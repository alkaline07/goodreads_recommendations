name: 0. Run Airflow Pipeline
run-name: Airflow Data Pipeline Execution

on:
  workflow_dispatch: 
    inputs:
      dag_id:
        description: 'DAG ID to trigger'
        required: true
        type: string
        default: 'goodreads_recommendation_pipeline'
      wait_for_completion:
        description: 'Wait for DAG to complete'
        required: false
        type: boolean
        default: true

env:
  COMPOSE_FILE: docker-compose.yaml

jobs:
  run-airflow-pipeline:
    runs-on: ubuntu-latest
    timeout-minutes: 90  
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up environment variables
        run: |
          echo "GIT_USER_NAME=${{ secrets.GIT_USER_NAME }}" >> $GITHUB_ENV
          echo "GIT_USER_EMAIL=${{ secrets.GIT_USER_EMAIL }}" >> $GITHUB_ENV
          echo "SMTP_PASSWORD=${{ secrets.SMTP_PASSWORD }}" >> $GITHUB_ENV

      - name: Create GCP credentials file
        run: |
          # Don't create config/dags - it already exists from git checkout
          mkdir -p logs plugins
          echo '${{ secrets.GCP_CREDENTIALS }}' > config/gcp_credentials.json
          chmod -R 777 logs plugins config
          
          # Verify DAG file exists
          if [ ! -f config/dags/data_pipeline_dag.py ]; then
            echo "ERROR: DAG file not found after checkout!"
            echo "Directory contents:"
            ls -la config/dags/
            exit 1
          fi
          
          echo "DAG file verified: config/dags/data_pipeline_dag.py"
          
          if [ ! -f config/gcp_credentials.json ]; then
            echo "ERROR: Failed to create GCP credentials file"
            exit 1
          fi
          
          if ! jq empty config/gcp_credentials.json 2>/dev/null; then
            echo "ERROR: GCP credentials file is not valid JSON"
            exit 1
          fi
          
          echo "GCP credentials file created and validated"

      - name: Build and start Airflow services
        run: |
          docker compose up -d --build

      - name: Wait for Airflow initialization
        run: |
          echo "Waiting for airflow-init to complete..."
          
          for i in {1..60}; do
            STATUS=$(docker inspect -f '{{.State.Status}}' airflow-init 2>/dev/null || echo "not found")
            
            if [ "$STATUS" = "exited" ]; then
              EXIT_CODE=$(docker inspect -f '{{.State.ExitCode}}' airflow-init)
              if [ "$EXIT_CODE" = "0" ]; then
                echo "Airflow initialization completed successfully"
                break
              else
                echo "ERROR: airflow-init failed with exit code $EXIT_CODE"
                docker logs airflow-init
                exit 1
              fi
            fi
            
            if [ $i -eq 60 ]; then
              echo "ERROR: airflow-init did not complete in time"
              docker logs airflow-init
              exit 1
            fi
            
            sleep 5
          done

      - name: Wait for Airflow webserver
        run: |
          echo "Waiting for Airflow webserver to be ready..."
          timeout 300 bash -c 'until curl -sf http://localhost:8080/health > /dev/null; do sleep 5; done'
          echo "Airflow webserver is ready"

      - name: Setup ngrok tunnel
        run: |
          curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null
          echo "deb https://ngrok-agent.s3.amazonaws.com buster main" | sudo tee /etc/apt/sources.list.d/ngrok.list
          sudo apt update && sudo apt install ngrok -y
          
          ngrok config add-authtoken ${{ secrets.NGROK_AUTH_TOKEN }}
          ngrok http 8080 --log=stdout > ngrok.log &
          
          sleep 10
          
          NGROK_URL=$(curl -s http://localhost:4040/api/tunnels | jq -r '.tunnels[0].public_url')
          
          if [ -z "$NGROK_URL" ] || [ "$NGROK_URL" = "null" ]; then
            echo "ERROR: Failed to get ngrok URL"
            cat ngrok.log
            exit 1
          fi
          
          echo "NGROK_URL=$NGROK_URL" >> $GITHUB_ENV
          
          echo "=========================================="
          echo "Airflow UI is now publicly accessible!"
          echo "=========================================="
          echo "URL:      $NGROK_URL"
          echo "Username: admin"
          echo "Password: admin"
          echo "=========================================="
          
          echo "::notice title=Airflow UI Access::Access Airflow at $NGROK_URL (user: admin, pass: admin)"

      - name: Verify DAG files and folder structure
        run: |
          echo "Checking DAG folder structure in container..."
          docker exec airflow-webserver ls -la /opt/airflow/config/dags/ || echo "DAG folder not found!"
          echo ""
          echo "Checking AIRFLOW__CORE__DAGS_FOLDER setting..."
          docker exec airflow-webserver printenv | grep DAGS_FOLDER
          echo ""
          echo "Checking for Python errors in DAG files..."
          docker exec airflow-webserver python -m py_compile /opt/airflow/config/dags/*.py 2>&1 || echo "DAG has syntax errors!"

      - name: List available DAGs
        run: |
          echo "Waiting for DAG to be parsed..."
          sleep 10
          echo ""
          echo "Available DAGs:"
          docker exec airflow-webserver airflow dags list
          echo ""
          echo "Checking DAG import errors..."
          docker exec airflow-webserver airflow dags list-import-errors || echo "No import errors"

      - name: Trigger DAG
        id: trigger_dag
        run: |
          # Set DAG ID based on trigger type
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            DAG_ID="${{ inputs.dag_id }}"
          else
            DAG_ID="goodreads_recommendation_pipeline"
          fi
          
          echo "Triggering DAG: $DAG_ID"
          
          for i in {1..12}; do
            if docker exec airflow-webserver airflow dags list | grep -q "$DAG_ID"; then
              echo "DAG found!"
              break
            fi
            echo "Waiting for DAG to be parsed... (attempt $i/12)"
            sleep 5
          done

          echo "Unpausing DAG..."
          docker exec airflow-webserver airflow dags unpause $DAG_ID
          
          echo "Trigger output: $TRIGGER_OUTPUT"

          RUN_ID=$(echo "$TRIGGER_OUTPUT" | grep -o '"dag_run_id": "[^"]*"' | cut -d'"' -f4)
          echo "run_id=$RUN_ID" >> $GITHUB_OUTPUT
          echo "dag_id=$DAG_ID" >> $GITHUB_OUTPUT
          echo "DAG triggered with run_id: $RUN_ID"
          echo ""
          echo "Monitor progress at: $NGROK_URL/dags/$DAG_ID/grid"
          echo "::notice title=DAG Triggered::Monitor at $NGROK_URL/dags/$DAG_ID/grid"

      - name: Wait for DAG completion
        if: ${{ github.event_name == 'workflow_dispatch' && inputs.wait_for_completion || github.event_name == 'push' }}
        run: |
          DAG_ID="${{ steps.trigger_dag.outputs.dag_id }}"
          
          echo "Waiting for DAG run ${{ steps.trigger_dag.outputs.run_id }} to complete..."
          echo "Live monitoring: $NGROK_URL/dags/$DAG_ID/grid"
          echo ""
          
          MAX_WAIT=3600
          ELAPSED=0
          INTERVAL=15
          
          while [ $ELAPSED -lt $MAX_WAIT ]; do
            # Get task states
            TASK_STATES=$(docker exec airflow-scheduler airflow tasks states-for-dag-run \
              $DAG_ID \
              ${{ steps.trigger_dag.outputs.run_id }} 2>/dev/null)
            
            # Count task states (FIX: strip newlines and spaces)
            SUCCESS_TASKS=$(echo "$TASK_STATES" | grep -c "success" 2>/dev/null | tr -d ' \n' || echo "0")
            FAILED_TASKS=$(echo "$TASK_STATES" | grep -c "failed" 2>/dev/null | tr -d ' \n' || echo "0")
            RUNNING_TASKS=$(echo "$TASK_STATES" | grep -c "running" 2>/dev/null | tr -d ' \n' || echo "0")
            TOTAL_TASKS=$(echo "$TASK_STATES" | wc -l | tr -d ' \n' || echo "0")
            
            # Ensure variables are numeric (default to 0 if empty)
            SUCCESS_TASKS=${SUCCESS_TASKS:-0}
            FAILED_TASKS=${FAILED_TASKS:-0}
            RUNNING_TASKS=${RUNNING_TASKS:-0}
            TOTAL_TASKS=${TOTAL_TASKS:-0}
            
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "Time elapsed: ${ELAPSED}s / ${MAX_WAIT}s"
            echo "Tasks: $SUCCESS_TASKS success, $RUNNING_TASKS running, $FAILED_TASKS failed (Total: $TOTAL_TASKS)"
            echo ""
            echo "Task status:"
            echo "$TASK_STATES" | head -20
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo ""
            
            # Check if any tasks failed
            if [ "$FAILED_TASKS" -gt 0 ]; then
              echo "DAG failed! $FAILED_TASKS task(s) failed"
              echo ""
              echo "Failed tasks:"
              echo "$TASK_STATES" | grep failed
              exit 1
            fi
            
            # Check if all tasks succeeded (13 tasks in your DAG)
            if [ "$SUCCESS_TASKS" -ge 13 ] && [ "$RUNNING_TASKS" -eq 0 ]; then
              echo "DAG completed successfully!"
              echo ""
              docker exec airflow-scheduler airflow dags list-runs -d $DAG_ID --state success --limit 1 || true
              exit 0
            fi
            
            sleep $INTERVAL
            ELAPSED=$((ELAPSED + INTERVAL))
          done
          
          echo ""
          echo "Timeout waiting for DAG completion after ${MAX_WAIT}s"
          echo "Current status: $SUCCESS_TASKS success, $RUNNING_TASKS running, $FAILED_TASKS failed"
          echo "Check status at: $NGROK_URL/dags/$DAG_ID/grid"
          exit 1

      - name: Show pipeline summary
        if: success()
        run: |
          DAG_ID="${{ steps.trigger_dag.outputs.dag_id }}"
          
          echo "=========================================="
          echo "Pipeline Execution Summary"
          echo "=========================================="
          echo "DAG: $DAG_ID"
          echo "Run ID: ${{ steps.trigger_dag.outputs.run_id }}"
          echo ""
          echo "All tasks completed:"
          docker exec airflow-webserver airflow tasks states-for-dag-run $DAG_ID ${{ steps.trigger_dag.outputs.run_id }}
          echo "=========================================="

      - name: Show failure details
        if: failure()
        run: |
          DAG_ID="${{ steps.trigger_dag.outputs.dag_id }}"
          
          echo "=========================================="
          echo "PIPELINE FAILURE DETAILS"
          echo "=========================================="
          
          echo "=== Container Status ==="
          docker ps -a
          echo ""
          
          # Check each container
          for container in airflow-webserver airflow-scheduler airflow-worker; do
            if docker ps -a --format '{{.Names}}' | grep -q "^${container}$"; then
              STATUS=$(docker inspect -f '{{.State.Status}}' $container 2>/dev/null)
              echo "=== $container (Status: $STATUS) ==="
              docker logs $container --tail 100 2>&1 || echo "Could not get logs"
              echo ""
            fi
          done
          
          echo "=== Failed Tasks ==="
          docker exec airflow-webserver airflow tasks states-for-dag-run $DAG_ID ${{ steps.trigger_dag.outputs.run_id }} 2>&1 | grep -i failed || echo "Cannot access task states"
          echo ""
          echo "Full logs available at: $NGROK_URL"
          echo "=========================================="

      - name: Keep ngrok alive for debugging
        if: failure()
        run: |
          DAG_ID="${{ steps.trigger_dag.outputs.dag_id }}"
          
          echo "=========================================="
          echo "Workflow failed - keeping ngrok alive"
          echo "=========================================="
          echo "Access Airflow UI at: $NGROK_URL"
          echo "Direct link to DAG: $NGROK_URL/dags/$DAG_ID/grid"
          echo ""
          echo "Keeping tunnel open for 10 minutes for debugging..."
          echo "=========================================="
          sleep 600

      - name: Cleanup
        if: always()
        run: |
          echo "Cleaning up..."
          pkill ngrok || true
          docker compose down -v
          rm -f config/gcp_credentials.json
          echo "Cleanup complete"