{
  "audit_metadata": {
    "model_name": "boosted_tree_regressor_final",
    "timestamp": "2025-11-18T20:55:14.951452",
    "predictions_table": "recommendation-system-475301.books.boosted_tree_rating_predictions",
    "debiased_table": "recommendation-system-475301.books.boosted_tree_regressor_final_rating_predictions_debiased"
  },
  "bias_detection": {
    "timestamp": "2025-11-18T20:55:26.034631",
    "total_slices_analyzed": 23,
    "disparities_found": 2,
    "high_risk_slices": 3,
    "recommendations": [
      "HIGH PRIORITY: 2 dimensions show severe performance disparities. Dimensions: Book Era, Rating Range",
      "Target these high-error slices for mitigation: Rating Range=Medium (3-4) (MAE: 3.444), Book Era=classic (MAE: 1.389), Book Era=modern (MAE: 1.327)",
      "Book Era: Consider re-weighting training data or adjusting decision thresholds to balance performance across 4 groups",
      "Rating Range: Consider re-weighting training data or adjusting decision thresholds to balance performance across 2 groups"
    ]
  },
  "mitigation_applied": {
    "techniques": [
      "prediction_shrinkage"
    ],
    "results": [
      {
        "technique": "prediction_shrinkage",
        "output_table": "recommendation-system-475301.books.boosted_tree_regressor_final_rating_predictions_debiased",
        "improvements": {
          "mae_change": 1.8510777086916244,
          "disparity_reduction": "See validation step"
        }
      }
    ]
  },
  "validation": {
    "timestamp": "2025-11-18T20:55:35.246723",
    "techniques_applied": [
      "prediction_shrinkage"
    ],
    "effectiveness": {
      "prediction_shrinkage": {
        "improvements": {
          "mae_change": 1.8510777086916244,
          "disparity_reduction": "See validation step"
        },
        "output_table": "recommendation-system-475301.books.boosted_tree_regressor_final_rating_predictions_debiased",
        "post_mitigation_disparities": [
          {
            "dimension": "Book Era",
            "severity": "medium",
            "mae_range": 0.44143821617824874,
            "mae_cv": 0.14478975574922892,
            "recommendation": "Significant performance disparity detected in Book Era. Consider bias mitigation."
          },
          {
            "dimension": "Rating Range",
            "severity": "high",
            "mae_range": 2.223007359891456,
            "mae_cv": 0.4602416107523088,
            "recommendation": "Significant performance disparity detected in Rating Range. Consider bias mitigation."
          }
        ]
      }
    }
  },
  "executive_summary": {
    "bias_detected": true,
    "severity": "high",
    "dimensions_with_bias": [
      "Book Era",
      "Rating Range"
    ],
    "mitigation_applied": true,
    "overall_status": "MITIGATED"
  }
}