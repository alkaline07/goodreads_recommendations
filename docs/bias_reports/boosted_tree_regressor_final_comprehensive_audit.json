{
  "audit_metadata": {
    "model_name": "boosted_tree_regressor_final",
    "timestamp": "2025-11-12T22:09:03.399068",
    "predictions_table": "recommendation-system-475301.books.boosted_tree_rating_predictions",
    "debiased_table": "recommendation-system-475301.books.boosted_tree_regressor_final_rating_predictions_debiased"
  },
  "bias_detection": {
    "timestamp": "2025-11-12T22:09:07.706671",
    "total_slices_analyzed": 20,
    "disparities_found": 2,
    "high_risk_slices": 2,
    "recommendations": [
      "HIGH PRIORITY: 1 dimensions show severe performance disparities. Dimensions: Rating Range",
      "MEDIUM PRIORITY: 1 dimensions show moderate disparities. Dimensions: Book Era",
      "Target these high-error slices for mitigation: Rating Range=Medium (3-4) (MAE: 3.751), Book Era=classic (MAE: 1.521)",
      "Book Era: Consider re-weighting training data or adjusting decision thresholds to balance performance across 4 groups",
      "Rating Range: Consider re-weighting training data or adjusting decision thresholds to balance performance across 2 groups"
    ]
  },
  "mitigation_applied": {
    "techniques": [
      "prediction_shrinkage"
    ],
    "results": [
      {
        "technique": "prediction_shrinkage",
        "output_table": "recommendation-system-475301.books.boosted_tree_regressor_final_rating_predictions_debiased",
        "improvements": {
          "mae_change": 1.2867160997993354,
          "disparity_reduction": "See validation step"
        }
      }
    ]
  },
  "validation": {
    "timestamp": "2025-11-12T22:09:14.181847",
    "techniques_applied": [
      "prediction_shrinkage"
    ],
    "effectiveness": {
      "prediction_shrinkage": {
        "improvements": {
          "mae_change": 1.2867160997993354,
          "disparity_reduction": "See validation step"
        },
        "output_table": "recommendation-system-475301.books.boosted_tree_regressor_final_rating_predictions_debiased",
        "post_mitigation_disparities": [
          {
            "dimension": "Book Era",
            "severity": "medium",
            "mae_range": 0.40333113300666756,
            "mae_cv": 0.11949891952871229,
            "recommendation": "Significant performance disparity detected in Book Era. Consider bias mitigation."
          },
          {
            "dimension": "Rating Range",
            "severity": "high",
            "mae_range": 2.411908241132875,
            "mae_cv": 0.4602549819959303,
            "recommendation": "Significant performance disparity detected in Rating Range. Consider bias mitigation."
          }
        ]
      }
    }
  },
  "executive_summary": {
    "bias_detected": true,
    "severity": "high",
    "dimensions_with_bias": [
      "Book Era",
      "Rating Range"
    ],
    "mitigation_applied": true,
    "overall_status": "MITIGATED"
  }
}