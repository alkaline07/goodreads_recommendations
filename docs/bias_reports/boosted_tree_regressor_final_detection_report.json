{
  "timestamp": "2025-11-18T20:55:26.034631",
  "model_name": "boosted_tree_regressor_final",
  "dataset": "test",
  "slice_metrics": [
    {
      "slice_name": "Popularity=High",
      "slice_dimension": "Popularity",
      "slice_value": "High",
      "count": 60639,
      "mae": 1.2859296422148476,
      "rmse": 1.6125425079267661,
      "mean_predicted": -0.8120981727384503,
      "mean_actual": -0.7430582907965295,
      "mean_error": 0.06903988194192079,
      "std_error": 1.6110771670554598
    },
    {
      "slice_name": "Popularity=Low",
      "slice_dimension": "Popularity",
      "slice_value": "Low",
      "count": 2779045,
      "mae": 1.2781579132325025,
      "rmse": 1.5822805431174118,
      "mean_predicted": -2.0093613103491967,
      "mean_actual": -2.084681911029173,
      "mean_error": -0.0753206006799581,
      "std_error": 1.5804870841260517
    },
    {
      "slice_name": "Popularity=Medium",
      "slice_dimension": "Popularity",
      "slice_value": "Medium",
      "count": 18770,
      "mae": 1.5121231449150072,
      "rmse": 1.7416236003707848,
      "mean_predicted": -1.4875184389701175,
      "mean_actual": -1.4261731242374256,
      "mean_error": 0.06134531473269217,
      "std_error": 1.7405892470487643
    },
    {
      "slice_name": "Book Length=long",
      "slice_dimension": "Book Length",
      "slice_value": "long",
      "count": 1149748,
      "mae": 1.2905739002899088,
      "rmse": 1.5905414410573944,
      "mean_predicted": -1.9118736384808064,
      "mean_actual": -1.9878764777107014,
      "mean_error": -0.07600283922989322,
      "std_error": 1.5887252246550378
    },
    {
      "slice_name": "Book Length=medium",
      "slice_dimension": "Book Length",
      "slice_value": "medium",
      "count": 1213389,
      "mae": 1.218346176703301,
      "rmse": 1.535660362685477,
      "mean_predicted": -2.163701513118838,
      "mean_actual": -2.2461390218451442,
      "mean_error": -0.08243750872630246,
      "std_error": 1.533446687891314
    },
    {
      "slice_name": "Book Length=short",
      "slice_dimension": "Book Length",
      "slice_value": "short",
      "count": 199390,
      "mae": 1.3732146357645705,
      "rmse": 1.654515874502595,
      "mean_predicted": -1.9769333969556608,
      "mean_actual": -1.9528540474561262,
      "mean_error": 0.024079349499533463,
      "std_error": 1.6543447917701324
    },
    {
      "slice_name": "Book Length=very_long",
      "slice_dimension": "Book Length",
      "slice_value": "very_long",
      "count": 295927,
      "mae": 1.4275497708694125,
      "rmse": 1.7010764265103475,
      "mean_predicted": -1.4986999749897654,
      "mean_actual": -1.5709126510211682,
      "mean_error": -0.07221267603140395,
      "std_error": 1.6995458507949797
    },
    {
      "slice_name": "Book Era=classic",
      "slice_dimension": "Book Era",
      "slice_value": "classic",
      "count": 707249,
      "mae": 1.389101877345547,
      "rmse": 1.667918603167345,
      "mean_predicted": -1.50061533077998,
      "mean_actual": -1.511500877530915,
      "mean_error": -0.010885546750937577,
      "std_error": 1.6678842600688566
    },
    {
      "slice_name": "Book Era=contemporary",
      "slice_dimension": "Book Era",
      "slice_value": "contemporary",
      "count": 564316,
      "mae": 1.0092741409072896,
      "rmse": 1.3484101056751732,
      "mean_predicted": -2.6858733153593066,
      "mean_actual": -2.826457291154218,
      "mean_error": -0.14058397579491264,
      "std_error": 1.341062692714495
    },
    {
      "slice_name": "Book Era=modern",
      "slice_dimension": "Book Era",
      "slice_value": "modern",
      "count": 1586851,
      "mae": 1.3274135119552926,
      "rmse": 1.6227461284298461,
      "mean_predicted": -1.9435651144066475,
      "mean_actual": -2.017254578393007,
      "mean_error": -0.07368946398636732,
      "std_error": 1.6210726437343117
    },
    {
      "slice_name": "Book Era=recent",
      "slice_dimension": "Book Era",
      "slice_value": "recent",
      "count": 38,
      "mae": 0.5389045466538523,
      "rmse": 0.766215506663949,
      "mean_predicted": -3.495306451069681,
      "mean_actual": -3.8794960052125496,
      "mean_error": -0.38418955414286815,
      "std_error": 0.6718352112002164
    },
    {
      "slice_name": "Genre Diversity=Multi-genre",
      "slice_dimension": "Genre Diversity",
      "slice_value": "Multi-genre",
      "count": 2858241,
      "mae": 1.2798752134784355,
      "rmse": 1.5840340384210436,
      "mean_predicted": -1.9804811536697289,
      "mean_actual": -2.0518729638515025,
      "mean_error": -0.07139181018178935,
      "std_error": 1.5824246965977562
    },
    {
      "slice_name": "Genre Diversity=Some genres",
      "slice_dimension": "Genre Diversity",
      "slice_value": "Some genres",
      "count": 213,
      "mae": 1.0637884305322043,
      "rmse": 1.4823184906842173,
      "mean_predicted": -2.716487180581535,
      "mean_actual": -2.3708478952176226,
      "mean_error": 0.34563928536391236,
      "std_error": 1.444853813762569
    },
    {
      "slice_name": "User Activity=Low",
      "slice_dimension": "User Activity",
      "slice_value": "Low",
      "count": 2858454,
      "mae": 1.279859111597925,
      "rmse": 1.584026702336614,
      "mean_predicted": -1.9805359977510708,
      "mean_actual": -2.05189673252521,
      "mean_error": -0.071360734774121,
      "std_error": 1.5824187547116224
    },
    {
      "slice_name": "Reading Pace=long_read",
      "slice_dimension": "Reading Pace",
      "slice_value": "long_read",
      "count": 1169022,
      "mae": 1.3685922535793984,
      "rmse": 1.654020848339684,
      "mean_predicted": -1.726289745635006,
      "mean_actual": -1.7803643501929949,
      "mean_error": -0.05407460455798219,
      "std_error": 1.6531373934488565
    },
    {
      "slice_name": "Reading Pace=moderate",
      "slice_dimension": "Reading Pace",
      "slice_value": "moderate",
      "count": 1140434,
      "mae": 1.2354404647831865,
      "rmse": 1.5440616756438474,
      "mean_predicted": -2.1432139013713596,
      "mean_actual": -2.224992849827973,
      "mean_error": -0.0817789484566105,
      "std_error": 1.5418951801150775
    },
    {
      "slice_name": "Reading Pace=quick_read",
      "slice_dimension": "Reading Pace",
      "slice_value": "quick_read",
      "count": 360065,
      "mae": 1.0873243235774583,
      "rmse": 1.4241505520831612,
      "mean_predicted": -2.4639492933914293,
      "mean_actual": -2.541504162669318,
      "mean_error": -0.07755486927788319,
      "std_error": 1.4220392587570068
    },
    {
      "slice_name": "Reading Pace=very_long",
      "slice_dimension": "Reading Pace",
      "slice_value": "very_long",
      "count": 188933,
      "mae": 1.3658714990826601,
      "rmse": 1.6667172102548826,
      "mean_predicted": -1.6504498905553824,
      "mean_actual": -1.7540775685033099,
      "mean_error": -0.10362767794792882,
      "std_error": 1.6634969822342531
    },
    {
      "slice_name": "Author Gender=Female",
      "slice_dimension": "Author Gender",
      "slice_value": "Female",
      "count": 1140853,
      "mae": 1.2502114104464188,
      "rmse": 1.5593261747086355,
      "mean_predicted": -2.1256873368376037,
      "mean_actual": -2.2138356551985563,
      "mean_error": -0.08814831836095473,
      "std_error": 1.5568333621781787
    },
    {
      "slice_name": "Author Gender=Male",
      "slice_dimension": "Author Gender",
      "slice_value": "Male",
      "count": 1355558,
      "mae": 1.3170764072948857,
      "rmse": 1.6142982446829495,
      "mean_predicted": -1.8015443879434785,
      "mean_actual": -1.847974823033356,
      "mean_error": -0.046430435089872345,
      "std_error": 1.6136309857957205
    },
    {
      "slice_name": "Author Gender=Unknown",
      "slice_dimension": "Author Gender",
      "slice_value": "Unknown",
      "count": 362043,
      "mae": 1.2339349144208565,
      "rmse": 1.5463822396398157,
      "mean_predicted": -2.1933205784685232,
      "mean_actual": -2.305124857466926,
      "mean_error": -0.11180427899839779,
      "std_error": 1.5423373187385172
    },
    {
      "slice_name": "Rating Range=Low (1-3)",
      "slice_dimension": "Rating Range",
      "slice_value": "Low (1-3)",
      "count": 2858441,
      "mae": 1.279849269670569,
      "rmse": 1.584013226996302,
      "mean_predicted": -1.9805434325816285,
      "mean_actual": -2.051920154541318,
      "mean_error": -0.07137672195968373,
      "std_error": 1.5824045446297745
    },
    {
      "slice_name": "Rating Range=Medium (3-4)",
      "slice_dimension": "Rating Range",
      "slice_value": "Medium (3-4)",
      "count": 13,
      "mae": 3.4439028565736627,
      "rmse": 3.4490451697502853,
      "mean_predicted": -0.3457648834356895,
      "mean_actual": 3.0981379731379732,
      "mean_error": 3.4439028565736627,
      "std_error": 0.19595791788885492
    }
  ],
  "disparity_analysis": {
    "summary": {
      "Popularity": {
        "mae_range": 0.23396523168250472,
        "rmse_range": 0.15934305725337294,
        "mae_coefficient_of_variation": 0.07985861781562557,
        "num_slices": 3,
        "max_mae_slice": "Medium",
        "min_mae_slice": "Low"
      },
      "Book Length": {
        "mae_range": 0.20920359416611145,
        "rmse_range": 0.1654160638248705,
        "mae_coefficient_of_variation": 0.060005177564693694,
        "num_slices": 4,
        "max_mae_slice": "very_long",
        "min_mae_slice": "medium"
      },
      "Book Era": {
        "mae_range": 0.8501973306916947,
        "rmse_range": 0.901703096503396,
        "mae_coefficient_of_variation": 0.3159107389800654,
        "num_slices": 4,
        "max_mae_slice": "classic",
        "min_mae_slice": "recent"
      },
      "Genre Diversity": {
        "mae_range": 0.21608678294623118,
        "rmse_range": 0.10171554773682634,
        "mae_coefficient_of_variation": 0.09220042453551419,
        "num_slices": 2,
        "max_mae_slice": "Multi-genre",
        "min_mae_slice": "Some genres"
      },
      "Reading Pace": {
        "mae_range": 0.28126793000194006,
        "rmse_range": 0.2425666581717214,
        "mae_coefficient_of_variation": 0.09134231923469421,
        "num_slices": 4,
        "max_mae_slice": "long_read",
        "min_mae_slice": "quick_read"
      },
      "Author Gender": {
        "mae_range": 0.08314149287402928,
        "rmse_range": 0.06791600504313378,
        "mae_coefficient_of_variation": 0.02839285559953224,
        "num_slices": 3,
        "max_mae_slice": "Male",
        "min_mae_slice": "Unknown"
      },
      "Rating Range": {
        "mae_range": 2.1640535869030937,
        "rmse_range": 1.8650319427539832,
        "mae_coefficient_of_variation": 0.45812174920865134,
        "num_slices": 2,
        "max_mae_slice": "Medium (3-4)",
        "min_mae_slice": "Low (1-3)"
      }
    },
    "detailed_disparities": [
      {
        "dimension": "Book Era",
        "severity": "high",
        "mae_range": 0.8501973306916947,
        "mae_cv": 0.3159107389800654,
        "recommendation": "Significant performance disparity detected in Book Era. Consider bias mitigation."
      },
      {
        "dimension": "Rating Range",
        "severity": "high",
        "mae_range": 2.1640535869030937,
        "mae_cv": 0.45812174920865134,
        "recommendation": "Significant performance disparity detected in Rating Range. Consider bias mitigation."
      }
    ],
    "high_risk_slices": [
      {
        "slice": "Book Era=classic",
        "dimension": "Book Era",
        "mae": 1.389101877345547,
        "mae_deviation_pct": 30.28853674471926,
        "count": 707249
      },
      {
        "slice": "Book Era=modern",
        "dimension": "Book Era",
        "mae": 1.3274135119552926,
        "mae_deviation_pct": 24.502577491515744,
        "count": 1586851
      },
      {
        "slice": "Rating Range=Medium (3-4)",
        "dimension": "Rating Range",
        "mae": 3.4439028565736627,
        "mae_deviation_pct": 45.812174920865125,
        "count": 13
      }
    ]
  },
  "recommendations": [
    "HIGH PRIORITY: 2 dimensions show severe performance disparities. Dimensions: Book Era, Rating Range",
    "Target these high-error slices for mitigation: Rating Range=Medium (3-4) (MAE: 3.444), Book Era=classic (MAE: 1.389), Book Era=modern (MAE: 1.327)",
    "Book Era: Consider re-weighting training data or adjusting decision thresholds to balance performance across 4 groups",
    "Rating Range: Consider re-weighting training data or adjusting decision thresholds to balance performance across 2 groups"
  ]
}