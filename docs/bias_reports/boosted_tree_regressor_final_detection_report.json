{
  "timestamp": "2025-12-10T04:17:09.766232",
  "model_name": "boosted_tree_regressor_final",
  "dataset": "test",
  "slice_metrics": [
    {
      "slice_name": "Popularity=High",
      "slice_dimension": "Popularity",
      "slice_value": "High",
      "count": 60639,
      "mae": 0.617529453808258,
      "rmse": 0.9088010754253457,
      "mean_predicted": -0.7511771295082941,
      "mean_actual": -0.7430582907965293,
      "mean_error": 0.008118838711764902,
      "std_error": 0.9087723029214838
    },
    {
      "slice_name": "Popularity=Low",
      "slice_dimension": "Popularity",
      "slice_value": "Low",
      "count": 2779045,
      "mae": 0.4483978513811732,
      "rmse": 0.8278132290292447,
      "mean_predicted": -2.031231044655133,
      "mean_actual": -2.084681911029177,
      "mean_error": -0.05345086637402282,
      "std_error": 0.8260859474643915
    },
    {
      "slice_name": "Popularity=Medium",
      "slice_dimension": "Popularity",
      "slice_value": "Medium",
      "count": 18770,
      "mae": 0.6338627041838287,
      "rmse": 1.0153320800168892,
      "mean_predicted": -1.418924458122035,
      "mean_actual": -1.4261731242374258,
      "mean_error": -0.007248666115390473,
      "std_error": 1.015333251886266
    },
    {
      "slice_name": "Book Length=long",
      "slice_dimension": "Book Length",
      "slice_value": "long",
      "count": 1149748,
      "mae": 0.467077795604,
      "rmse": 0.8489906812730462,
      "mean_predicted": -1.9331237430598285,
      "mean_actual": -1.9878764777107025,
      "mean_error": -0.05475273465087451,
      "std_error": 0.8472236654136568
    },
    {
      "slice_name": "Book Length=medium",
      "slice_dimension": "Book Length",
      "slice_value": "medium",
      "count": 1213389,
      "mae": 0.42038049763895446,
      "rmse": 0.794927507116643,
      "mean_predicted": -2.1932507040721587,
      "mean_actual": -2.2461390218451402,
      "mean_error": -0.0528883177729789,
      "std_error": 0.7931664931714094
    },
    {
      "slice_name": "Book Length=short",
      "slice_dimension": "Book Length",
      "slice_value": "short",
      "count": 199390,
      "mae": 0.4622652687965704,
      "rmse": 0.8329273901632577,
      "mean_predicted": -1.9113611847825713,
      "mean_actual": -1.9528540474561302,
      "mean_error": -0.04149286267356113,
      "std_error": 0.8318953362452683
    },
    {
      "slice_name": "Book Length=very_long",
      "slice_dimension": "Book Length",
      "slice_value": "very_long",
      "count": 295927,
      "mae": 0.5277783827211122,
      "rmse": 0.9006330541472503,
      "mean_predicted": -1.5277030416153121,
      "mean_actual": -1.5709126510211702,
      "mean_error": -0.04320960940585819,
      "std_error": 0.8995974447446471
    },
    {
      "slice_name": "Book Era=classic",
      "slice_dimension": "Book Era",
      "slice_value": "classic",
      "count": 707249,
      "mae": 0.48199156567678325,
      "rmse": 0.8229886543388696,
      "mean_predicted": -1.4750960200693362,
      "mean_actual": -1.5115008775309164,
      "mean_error": -0.03640485746157918,
      "std_error": 0.8221836579013174
    },
    {
      "slice_name": "Book Era=contemporary",
      "slice_dimension": "Book Era",
      "slice_value": "contemporary",
      "count": 564316,
      "mae": 0.37851760192358513,
      "rmse": 0.7838176504061586,
      "mean_predicted": -2.7576312912682264,
      "mean_actual": -2.826457291154223,
      "mean_error": -0.06882599988599737,
      "std_error": 0.7807907345340975
    },
    {
      "slice_name": "Book Era=modern",
      "slice_dimension": "Book Era",
      "slice_value": "modern",
      "count": 1586851,
      "mae": 0.46693697758195024,
      "rmse": 0.850590555218263,
      "mean_predicted": -1.9645773410108522,
      "mean_actual": -2.0172545783930245,
      "mean_error": -0.052677237382162664,
      "std_error": 0.8489580999537231
    },
    {
      "slice_name": "Book Era=recent",
      "slice_dimension": "Book Era",
      "slice_value": "recent",
      "count": 38,
      "mae": 0.2835975067216517,
      "rmse": 0.7747271483136102,
      "mean_predicted": -3.6620396603094902,
      "mean_actual": -3.87949600521255,
      "mean_error": -0.21745634490305918,
      "std_error": 0.7535638912102679
    },
    {
      "slice_name": "Genre Diversity=Multi-genre",
      "slice_dimension": "Genre Diversity",
      "slice_value": "Multi-genre",
      "count": 2858241,
      "mae": 0.4532043266071654,
      "rmse": 0.8309774240188637,
      "mean_predicted": -2.0000315329678133,
      "mean_actual": -2.05187296385153,
      "mean_error": -0.05184143088369553,
      "std_error": 0.8293589005509827
    },
    {
      "slice_name": "Genre Diversity=Some genres",
      "slice_dimension": "Genre Diversity",
      "slice_value": "Some genres",
      "count": 213,
      "mae": 0.4440878526235917,
      "rmse": 0.8659733383000878,
      "mean_predicted": -2.320148860974491,
      "mean_actual": -2.3708478952176226,
      "mean_error": -0.050699034243131824,
      "std_error": 0.8665244466064894
    },
    {
      "slice_name": "User Activity=Low",
      "slice_dimension": "User Activity",
      "slice_value": "Low",
      "count": 2858454,
      "mae": 0.45320364728577084,
      "rmse": 0.8309800866748857,
      "mean_predicted": -2.0000553867680995,
      "mean_actual": -2.051896732525216,
      "mean_error": -0.0518413457570909,
      "std_error": 0.8293615737144328
    },
    {
      "slice_name": "Reading Pace=long_read",
      "slice_dimension": "Reading Pace",
      "slice_value": "long_read",
      "count": 1169311,
      "mae": 0.49347073541306,
      "rmse": 0.8659038767279694,
      "mean_predicted": -1.7340063174738505,
      "mean_actual": -1.7805283849606763,
      "mean_error": -0.04652206748682596,
      "std_error": 0.8646536071411832
    },
    {
      "slice_name": "Reading Pace=moderate",
      "slice_dimension": "Reading Pace",
      "slice_value": "moderate",
      "count": 1140303,
      "mae": 0.4269908179485386,
      "rmse": 0.8096844173191393,
      "mean_predicted": -2.168041536123413,
      "mean_actual": -2.2249281186574437,
      "mean_error": -0.056886582534029756,
      "std_error": 0.8076839384714504
    },
    {
      "slice_name": "Reading Pace=quick_read",
      "slice_dimension": "Reading Pace",
      "slice_value": "quick_read",
      "count": 359918,
      "mae": 0.3751064892262957,
      "rmse": 0.7421641814874694,
      "mean_predicted": -2.4877846848598213,
      "mean_actual": -2.5416623199039474,
      "mean_error": -0.053877635044123406,
      "std_error": 0.7402069947210548
    },
    {
      "slice_name": "Reading Pace=very_long",
      "slice_dimension": "Reading Pace",
      "slice_value": "very_long",
      "count": 188922,
      "mae": 0.5109755943378593,
      "rmse": 0.8948767787623287,
      "mean_predicted": -1.7036179135604876,
      "mean_actual": -1.754050713957156,
      "mean_error": -0.05043280039666633,
      "std_error": 0.8934568860227273
    },
    {
      "slice_name": "Author Gender=Female",
      "slice_dimension": "Author Gender",
      "slice_value": "Female",
      "count": 1140853,
      "mae": 0.43351538450470295,
      "rmse": 0.8191885394147923,
      "mean_predicted": -2.154996814912263,
      "mean_actual": -2.213835655198571,
      "mean_error": -0.058838840286307974,
      "std_error": 0.8170730929151204
    },
    {
      "slice_name": "Author Gender=Male",
      "slice_dimension": "Author Gender",
      "slice_value": "Male",
      "count": 1355558,
      "mae": 0.47865574546199335,
      "rmse": 0.847426683838737,
      "mean_predicted": -1.8036000722863794,
      "mean_actual": -1.8479748230333515,
      "mean_error": -0.04437475074697182,
      "std_error": 0.846264376122715
    },
    {
      "slice_name": "Author Gender=Unknown",
      "slice_dimension": "Author Gender",
      "slice_value": "Unknown",
      "count": 362043,
      "mae": 0.41994687491067595,
      "rmse": 0.8052876745441275,
      "mean_predicted": -2.247377334894966,
      "mean_actual": -2.3051248574669216,
      "mean_error": -0.057747522571961016,
      "std_error": 0.8032155653338809
    },
    {
      "slice_name": "Rating Range=Low (1-3)",
      "slice_dimension": "Rating Range",
      "slice_value": "Low (1-3)",
      "count": 2858441,
      "mae": 0.453191465888831,
      "rmse": 0.8309550666752511,
      "mean_predicted": -2.00006433047617,
      "mean_actual": -2.051920154541328,
      "mean_error": -0.05185582406514851,
      "std_error": 0.8293355997204059
    },
    {
      "slice_name": "Rating Range=Medium (3-4)",
      "slice_dimension": "Rating Range",
      "slice_value": "Medium (3-4)",
      "count": 13,
      "mae": 3.1316501436361603,
      "rmse": 3.1358407219090223,
      "mean_predicted": -0.03351217049818773,
      "mean_actual": 3.0981379731379732,
      "mean_error": 3.1316501436361603,
      "std_error": 0.1686805026286555
    }
  ],
  "disparity_analysis": {
    "summary": {
      "Popularity": {
        "mae_range": 0.1854648528026555,
        "rmse_range": 0.18751885098764454,
        "mae_coefficient_of_variation": 0.14797961039302765,
        "num_slices": 3,
        "max_mae_slice": "Medium",
        "min_mae_slice": "Low"
      },
      "Book Length": {
        "mae_range": 0.10739788508215775,
        "rmse_range": 0.1057055470306073,
        "mae_coefficient_of_variation": 0.0815955724152241,
        "num_slices": 4,
        "max_mae_slice": "very_long",
        "min_mae_slice": "medium"
      },
      "Book Era": {
        "mae_range": 0.19839405895513157,
        "rmse_range": 0.07586340690465276,
        "mae_coefficient_of_variation": 0.19700743704682402,
        "num_slices": 4,
        "max_mae_slice": "classic",
        "min_mae_slice": "recent"
      },
      "Genre Diversity": {
        "mae_range": 0.009116473983573736,
        "rmse_range": 0.03499591428122417,
        "mae_coefficient_of_variation": 0.01015998377628704,
        "num_slices": 2,
        "max_mae_slice": "Multi-genre",
        "min_mae_slice": "Some genres"
      },
      "Reading Pace": {
        "mae_range": 0.13586910511156364,
        "rmse_range": 0.1527125972748593,
        "mae_coefficient_of_variation": 0.11993115991718162,
        "num_slices": 4,
        "max_mae_slice": "very_long",
        "min_mae_slice": "quick_read"
      },
      "Author Gender": {
        "mae_range": 0.058708870551317405,
        "rmse_range": 0.04213900929460945,
        "mae_coefficient_of_variation": 0.05651853839426101,
        "num_slices": 3,
        "max_mae_slice": "Male",
        "min_mae_slice": "Unknown"
      },
      "Rating Range": {
        "mae_range": 2.6784586777473294,
        "rmse_range": 2.3048856552337713,
        "mae_coefficient_of_variation": 0.7471623489948941,
        "num_slices": 2,
        "max_mae_slice": "Medium (3-4)",
        "min_mae_slice": "Low (1-3)"
      }
    },
    "detailed_disparities": [
      {
        "dimension": "Book Era",
        "severity": "medium",
        "mae_range": 0.19839405895513157,
        "mae_cv": 0.19700743704682402,
        "recommendation": "Significant performance disparity detected in Book Era. Consider bias mitigation."
      },
      {
        "dimension": "Rating Range",
        "severity": "high",
        "mae_range": 2.6784586777473294,
        "mae_cv": 0.7471623489948941,
        "recommendation": "Significant performance disparity detected in Rating Range. Consider bias mitigation."
      }
    ],
    "high_risk_slices": [
      {
        "slice": "Rating Range=Medium (3-4)",
        "dimension": "Rating Range",
        "mae": 3.1316501436361603,
        "mae_deviation_pct": 74.71623489948941,
        "count": 13
      }
    ]
  },
  "recommendations": [
    "HIGH PRIORITY: 1 dimensions show severe performance disparities. Dimensions: Rating Range",
    "MEDIUM PRIORITY: 1 dimensions show moderate disparities. Dimensions: Book Era",
    "Target these high-error slices for mitigation: Rating Range=Medium (3-4) (MAE: 3.132)",
    "Rating Range: Consider re-weighting training data or adjusting decision thresholds to balance performance across 2 groups"
  ]
}