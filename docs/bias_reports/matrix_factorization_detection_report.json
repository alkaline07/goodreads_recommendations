{
  "timestamp": "2025-12-08T15:04:20.428366",
  "model_name": "matrix_factorization",
  "dataset": "test",
  "slice_metrics": [
    {
      "slice_name": "Popularity=High",
      "slice_dimension": "Popularity",
      "slice_value": "High",
      "count": 60639,
      "mae": 1.368658059156926,
      "rmse": 1.7625714601118048,
      "mean_predicted": -0.6058251710729172,
      "mean_actual": -0.7430582907965294,
      "mean_error": -0.13723311972361166,
      "std_error": 1.757235369872722
    },
    {
      "slice_name": "Popularity=Low",
      "slice_dimension": "Popularity",
      "slice_value": "Low",
      "count": 2779072,
      "mae": 1.7326206029782658,
      "rmse": 3.8727980669771185,
      "mean_predicted": -2.0860852496382423,
      "mean_actual": -2.0846939223246936,
      "mean_error": 0.0013913273135634527,
      "std_error": 3.872798513834652
    },
    {
      "slice_name": "Popularity=Medium",
      "slice_dimension": "Popularity",
      "slice_value": "Medium",
      "count": 18770,
      "mae": 1.7411057988905054,
      "rmse": 2.030416799978694,
      "mean_predicted": -1.0499245831889439,
      "mean_actual": -1.4261731242374256,
      "mean_error": -0.3762485410484818,
      "std_error": 1.9953048698454061
    },
    {
      "slice_name": "Book Length=long",
      "slice_dimension": "Book Length",
      "slice_value": "long",
      "count": 1149762,
      "mae": 1.7436797887549662,
      "rmse": 2.026661458125764,
      "mean_predicted": -2.0041796707061947,
      "mean_actual": -1.9878922807458683,
      "mean_error": 0.01628738996033089,
      "std_error": 2.026596891073046
    },
    {
      "slice_name": "Book Length=medium",
      "slice_dimension": "Book Length",
      "slice_value": "medium",
      "count": 1213394,
      "mae": 1.696850290170065,
      "rmse": 5.391304575840464,
      "mean_predicted": -2.213936144614615,
      "mean_actual": -2.246143227105947,
      "mean_error": -0.0322070824913238,
      "std_error": 5.39121059567454
    },
    {
      "slice_name": "Book Length=short",
      "slice_dimension": "Book Length",
      "slice_value": "short",
      "count": 199390,
      "mae": 1.7431465759291287,
      "rmse": 1.9743714044338008,
      "mean_predicted": -1.906183423200845,
      "mean_actual": -1.9528540474561282,
      "mean_error": -0.0466706242552817,
      "std_error": 1.9738246717964125
    },
    {
      "slice_name": "Book Length=very_long",
      "slice_dimension": "Book Length",
      "slice_value": "very_long",
      "count": 295935,
      "mae": 1.7551871127469916,
      "rmse": 1.9963420348052068,
      "mean_predicted": -1.6322653633291166,
      "mean_actual": -1.5709625472217712,
      "mean_error": 0.061302816107345746,
      "std_error": 1.9954039538645223
    },
    {
      "slice_name": "Book Era=classic",
      "slice_dimension": "Book Era",
      "slice_value": "classic",
      "count": 707249,
      "mae": 1.8184433124545398,
      "rmse": 2.0065329932851714,
      "mean_predicted": -1.6384387481950151,
      "mean_actual": -1.511500877530918,
      "mean_error": 0.1269378706640969,
      "std_error": 2.002515193471475
    },
    {
      "slice_name": "Book Era=contemporary",
      "slice_dimension": "Book Era",
      "slice_value": "contemporary",
      "count": 564320,
      "mae": 1.5228788177981822,
      "rmse": 2.40276632767137,
      "mean_predicted": -2.6463404614086077,
      "mean_actual": -2.8264641327290394,
      "mean_error": -0.18012367132043378,
      "std_error": 2.396007441863337
    },
    {
      "slice_name": "Book Era=modern",
      "slice_dimension": "Book Era",
      "slice_value": "modern",
      "count": 1586874,
      "mae": 1.7551696731538615,
      "rmse": 4.752550989775055,
      "mean_predicted": -2.0175118795720413,
      "mean_actual": -2.0172722881448455,
      "mean_error": 0.00023959142721692052,
      "std_error": 4.752552481193424
    },
    {
      "slice_name": "Book Era=recent",
      "slice_dimension": "Book Era",
      "slice_value": "recent",
      "count": 38,
      "mae": 0.9396733685027034,
      "rmse": 1.2215583709627427,
      "mean_predicted": -3.1753176189354906,
      "mean_actual": -3.87949600521255,
      "mean_error": -0.7041783862770591,
      "std_error": 1.011565963578877
    },
    {
      "slice_name": "Genre Diversity=Multi-genre",
      "slice_dimension": "Genre Diversity",
      "slice_value": "Multi-genre",
      "count": 2858268,
      "mae": 1.7249283621593074,
      "rmse": 3.830837873261257,
      "mean_predicted": -2.0478662430811587,
      "mean_actual": -2.0518849522642983,
      "mean_error": -0.004018709183136188,
      "std_error": 3.830836435496202
    },
    {
      "slice_name": "Genre Diversity=Some genres",
      "slice_dimension": "Genre Diversity",
      "slice_value": "Some genres",
      "count": 213,
      "mae": 2.0867436715325947,
      "rmse": 2.957618058091726,
      "mean_predicted": -2.2266337028418604,
      "mean_actual": -2.370847895217623,
      "mean_error": -0.14421419237576272,
      "std_error": 2.9610590260576104
    },
    {
      "slice_name": "User Activity=Low",
      "slice_dimension": "User Activity",
      "slice_value": "Low",
      "count": 2858481,
      "mae": 1.7249553228635786,
      "rmse": 3.8307802207301385,
      "mean_predicted": -2.0478795639564464,
      "mean_actual": -2.051908719820169,
      "mean_error": -0.004029155863705457,
      "std_error": 3.8307787718998996
    },
    {
      "slice_name": "Reading Pace=long_read",
      "slice_dimension": "Reading Pace",
      "slice_value": "long_read",
      "count": 1169207,
      "mae": 1.7692722346519834,
      "rmse": 1.9723356093370605,
      "mean_predicted": -1.8209845428956832,
      "mean_actual": -1.7806488417650297,
      "mean_error": 0.04033570113064736,
      "std_error": 1.971923962230313
    },
    {
      "slice_name": "Reading Pace=moderate",
      "slice_dimension": "Reading Pace",
      "slice_value": "moderate",
      "count": 1140260,
      "mae": 1.7230618808473763,
      "rmse": 5.416305779336917,
      "mean_predicted": -2.2178636263062232,
      "mean_actual": -2.2249320383905684,
      "mean_error": -0.007068412084348109,
      "std_error": 5.416303542139857
    },
    {
      "slice_name": "Reading Pace=quick_read",
      "slice_dimension": "Reading Pace",
      "slice_value": "quick_read",
      "count": 360041,
      "mae": 1.6151074563753662,
      "rmse": 2.993116387151689,
      "mean_predicted": -2.41691081475954,
      "mean_actual": -2.5409984088773774,
      "mean_error": -0.12408759411783352,
      "std_error": 2.9905472435484635
    },
    {
      "slice_name": "Reading Pace=very_long",
      "slice_dimension": "Reading Pace",
      "slice_value": "very_long",
      "count": 188973,
      "mae": 1.6714720655214554,
      "rmse": 1.9558842978022335,
      "mean_predicted": -1.7229381140354283,
      "mean_actual": -1.754379645668114,
      "mean_error": -0.03144153163268726,
      "std_error": 1.9556367389928324
    },
    {
      "slice_name": "Author Gender=Female",
      "slice_dimension": "Author Gender",
      "slice_value": "Female",
      "count": 1140863,
      "mae": 1.7143607858428462,
      "rmse": 2.391592738132239,
      "mean_predicted": -2.184329489095268,
      "mean_actual": -2.213846636631146,
      "mean_error": -0.029517147535875858,
      "std_error": 2.3914116282706748
    },
    {
      "slice_name": "Author Gender=Male",
      "slice_dimension": "Author Gender",
      "slice_value": "Male",
      "count": 1355575,
      "mae": 1.7434687029654086,
      "rmse": 5.013632188033072,
      "mean_predicted": -1.8747462860861506,
      "mean_actual": -1.8479922211323132,
      "mean_error": 0.026754064953833995,
      "std_error": 5.013562653386823
    },
    {
      "slice_name": "Author Gender=Unknown",
      "slice_dimension": "Author Gender",
      "slice_value": "Unknown",
      "count": 362043,
      "mae": 1.689022132766094,
      "rmse": 1.9296062471148805,
      "mean_predicted": -2.2661532833914317,
      "mean_actual": -2.305124857466927,
      "mean_error": -0.0389715740754885,
      "std_error": 1.929215323781003
    },
    {
      "slice_name": "Rating Range=Low (1-3)",
      "slice_dimension": "Rating Range",
      "slice_value": "Low (1-3)",
      "count": 2858468,
      "mae": 1.724943452451354,
      "rmse": 3.8307775704425695,
      "mean_predicted": -2.047883252161226,
      "mean_actual": -2.051932141669566,
      "mean_error": -0.004048889508341676,
      "std_error": 3.830776100807025
    },
    {
      "slice_name": "Rating Range=Medium (3-4)",
      "slice_dimension": "Rating Range",
      "slice_value": "Medium (3-4)",
      "count": 13,
      "mae": 4.335047129982882,
      "rmse": 4.374889386297868,
      "mean_predicted": -1.2369091568449089,
      "mean_actual": 3.0981379731379732,
      "mean_error": 4.335047129982882,
      "std_error": 0.6131412154631721
    }
  ],
  "disparity_analysis": {
    "summary": {
      "Popularity": {
        "mae_range": 0.3724477397335795,
        "rmse_range": 2.1102266068653135,
        "mae_coefficient_of_variation": 0.10755535820040749,
        "num_slices": 3,
        "max_mae_slice": "Medium",
        "min_mae_slice": "High"
      },
      "Book Length": {
        "mae_range": 0.05833682257692652,
        "rmse_range": 3.4169331714066633,
        "mae_coefficient_of_variation": 0.012903967928898176,
        "num_slices": 4,
        "max_mae_slice": "very_long",
        "min_mae_slice": "medium"
      },
      "Book Era": {
        "mae_range": 0.8787699439518364,
        "rmse_range": 3.5309926188123124,
        "mae_coefficient_of_variation": 0.2297190183594458,
        "num_slices": 4,
        "max_mae_slice": "classic",
        "min_mae_slice": "recent"
      },
      "Genre Diversity": {
        "mae_range": 0.3618153093732872,
        "rmse_range": 0.8732198151695307,
        "mae_coefficient_of_variation": 0.09492299079646703,
        "num_slices": 2,
        "max_mae_slice": "Some genres",
        "min_mae_slice": "Multi-genre"
      },
      "Reading Pace": {
        "mae_range": 0.15416477827661712,
        "rmse_range": 3.4604214815346834,
        "mae_coefficient_of_variation": 0.03394787500060149,
        "num_slices": 4,
        "max_mae_slice": "long_read",
        "min_mae_slice": "quick_read"
      },
      "Author Gender": {
        "mae_range": 0.05444657019931465,
        "rmse_range": 3.084025940918192,
        "mae_coefficient_of_variation": 0.012966451271486322,
        "num_slices": 3,
        "max_mae_slice": "Male",
        "min_mae_slice": "Unknown"
      },
      "Rating Range": {
        "mae_range": 2.6101036775315274,
        "rmse_range": 0.5441118158552984,
        "mae_coefficient_of_variation": 0.4307108471582931,
        "num_slices": 2,
        "max_mae_slice": "Medium (3-4)",
        "min_mae_slice": "Low (1-3)"
      }
    },
    "detailed_disparities": [
      {
        "dimension": "Popularity",
        "severity": "medium",
        "mae_range": 0.3724477397335795,
        "mae_cv": 0.10755535820040749,
        "recommendation": "Significant performance disparity detected in Popularity. Consider bias mitigation."
      },
      {
        "dimension": "Book Era",
        "severity": "medium",
        "mae_range": 0.8787699439518364,
        "mae_cv": 0.2297190183594458,
        "recommendation": "Significant performance disparity detected in Book Era. Consider bias mitigation."
      },
      {
        "dimension": "Genre Diversity",
        "severity": "medium",
        "mae_range": 0.3618153093732872,
        "mae_cv": 0.09492299079646703,
        "recommendation": "Significant performance disparity detected in Genre Diversity. Consider bias mitigation."
      },
      {
        "dimension": "Rating Range",
        "severity": "high",
        "mae_range": 2.6101036775315274,
        "mae_cv": 0.4307108471582931,
        "recommendation": "Significant performance disparity detected in Rating Range. Consider bias mitigation."
      }
    ],
    "high_risk_slices": [
      {
        "slice": "Book Era=classic",
        "dimension": "Book Era",
        "mae": 1.8184433124545398,
        "mae_deviation_pct": 20.503217567146308,
        "count": 707249
      },
      {
        "slice": "Rating Range=Medium (3-4)",
        "dimension": "Rating Range",
        "mae": 4.335047129982882,
        "mae_deviation_pct": 43.07108471582931,
        "count": 13
      }
    ]
  },
  "recommendations": [
    "HIGH PRIORITY: 1 dimensions show severe performance disparities. Dimensions: Rating Range",
    "MEDIUM PRIORITY: 3 dimensions show moderate disparities. Dimensions: Popularity, Book Era, Genre Diversity",
    "Target these high-error slices for mitigation: Rating Range=Medium (3-4) (MAE: 4.335), Book Era=classic (MAE: 1.818)",
    "Book Era: Consider re-weighting training data or adjusting decision thresholds to balance performance across 4 groups",
    "Rating Range: Consider re-weighting training data or adjusting decision thresholds to balance performance across 2 groups"
  ]
}