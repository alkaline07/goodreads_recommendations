{
  "timestamp": "2025-11-18T20:54:47.357203",
  "model_name": "matrix_factorization",
  "dataset": "test",
  "slice_metrics": [
    {
      "slice_name": "Popularity=High",
      "slice_dimension": "Popularity",
      "slice_value": "High",
      "count": 60639,
      "mae": 1.3564340314890073,
      "rmse": 1.7608131627928385,
      "mean_predicted": -0.5732807010383785,
      "mean_actual": -0.7430582907965297,
      "mean_error": -0.1697775897581506,
      "std_error": 1.7526235247825972
    },
    {
      "slice_name": "Popularity=Low",
      "slice_dimension": "Popularity",
      "slice_value": "Low",
      "count": 2779045,
      "mae": 1.7405915677737889,
      "rmse": 4.002268048844178,
      "mean_predicted": -2.0763238068296044,
      "mean_actual": -2.0846819110291444,
      "mean_error": -0.008358104199554432,
      "std_error": 4.002260041623282
    },
    {
      "slice_name": "Popularity=Medium",
      "slice_dimension": "Popularity",
      "slice_value": "Medium",
      "count": 18770,
      "mae": 1.7796977896173916,
      "rmse": 2.0013141321764043,
      "mean_predicted": -1.270516969028633,
      "mean_actual": -1.4261731242374258,
      "mean_error": -0.15565615520879256,
      "std_error": 1.9953048698454061
    },
    {
      "slice_name": "Book Length=long",
      "slice_dimension": "Book Length",
      "slice_value": "long",
      "count": 1149748,
      "mae": 1.7467378628532297,
      "rmse": 2.3780421470281294,
      "mean_predicted": -1.9910988293708063,
      "mean_actual": -1.9878764777106999,
      "mean_error": 0.0032223516601027563,
      "std_error": 2.3780409979707535
    },
    {
      "slice_name": "Book Length=medium",
      "slice_dimension": "Book Length",
      "slice_value": "medium",
      "count": 1213389,
      "mae": 1.7073255347979448,
      "rmse": 5.2085710626477875,
      "mean_predicted": -2.207613742914954,
      "mean_actual": -2.2461390218451327,
      "mean_error": -0.03852527893017574,
      "std_error": 5.208430730512434
    },
    {
      "slice_name": "Book Length=short",
      "slice_dimension": "Book Length",
      "slice_value": "short",
      "count": 199390,
      "mae": 1.7536144139485217,
      "rmse": 1.989181222632706,
      "mean_predicted": -1.9175803991482097,
      "mean_actual": -1.9528540474561296,
      "mean_error": -0.03527364830792102,
      "std_error": 1.9888734360951756
    },
    {
      "slice_name": "Book Length=very_long",
      "slice_dimension": "Book Length",
      "slice_value": "very_long",
      "count": 295927,
      "mae": 1.7680997628826596,
      "rmse": 3.929405939213499,
      "mean_predicted": -1.6169716894041526,
      "mean_actual": -1.5709126510211704,
      "mean_error": 0.04605903838298025,
      "std_error": 3.929142625169967
    },
    {
      "slice_name": "Book Era=classic",
      "slice_dimension": "Book Era",
      "slice_value": "classic",
      "count": 707249,
      "mae": 1.8150926042163293,
      "rmse": 2.065257068515326,
      "mean_predicted": -1.6324665190370007,
      "mean_actual": -1.5115008775309153,
      "mean_error": 0.12096564150608502,
      "std_error": 2.061712900177879
    },
    {
      "slice_name": "Book Era=contemporary",
      "slice_dimension": "Book Era",
      "slice_value": "contemporary",
      "count": 564316,
      "mae": 1.5486895116929535,
      "rmse": 7.31168591226714,
      "mean_predicted": -2.629339297406215,
      "mean_actual": -2.8264572911542207,
      "mean_error": -0.19711799374800987,
      "std_error": 7.309034822942545
    },
    {
      "slice_name": "Book Era=modern",
      "slice_dimension": "Book Era",
      "slice_value": "modern",
      "count": 1586851,
      "mae": 1.7614306365099994,
      "rmse": 2.702888215064891,
      "mean_predicted": -2.0104932708059304,
      "mean_actual": -2.0172545783930187,
      "mean_error": -0.006761307587085246,
      "std_error": 2.7028806099545664
    },
    {
      "slice_name": "Book Era=recent",
      "slice_dimension": "Book Era",
      "slice_value": "recent",
      "count": 38,
      "mae": 1.0357240542309973,
      "rmse": 1.2878958211898577,
      "mean_predicted": -3.059482646649023,
      "mean_actual": -3.87949600521255,
      "mean_error": -0.8200133585635266,
      "std_error": 1.0064339245318188
    },
    {
      "slice_name": "Genre Diversity=Multi-genre",
      "slice_dimension": "Genre Diversity",
      "slice_value": "Multi-genre",
      "count": 2858241,
      "mae": 1.7327026434445458,
      "rmse": 3.958042214204614,
      "mean_predicted": -2.0391225502988193,
      "mean_actual": -2.051872963851516,
      "mean_error": -0.012750413552692443,
      "std_error": 3.9580223694861654
    },
    {
      "slice_name": "Genre Diversity=Some genres",
      "slice_dimension": "Genre Diversity",
      "slice_value": "Some genres",
      "count": 213,
      "mae": 1.6821028587484352,
      "rmse": 2.0116433676162377,
      "mean_predicted": -2.3680206623394224,
      "mean_actual": -2.3708478952176226,
      "mean_error": -0.002827232878200494,
      "std_error": 2.016380236390484
    },
    {
      "slice_name": "User Activity=Low",
      "slice_dimension": "User Activity",
      "slice_value": "Low",
      "count": 2858454,
      "mae": 1.7326988729608692,
      "rmse": 3.95793283688527,
      "mean_predicted": -2.0391470584062974,
      "mean_actual": -2.051896732525183,
      "mean_error": -0.01274967411889933,
      "std_error": 3.957912993910552
    },
    {
      "slice_name": "Reading Pace=long_read",
      "slice_dimension": "Reading Pace",
      "slice_value": "long_read",
      "count": 1169022,
      "mae": 1.7704100154727382,
      "rmse": 2.178253578766998,
      "mean_predicted": -1.8159830234351657,
      "mean_actual": -1.7803643501929913,
      "mean_error": 0.03561867324216673,
      "std_error": 2.1779632736144947
    },
    {
      "slice_name": "Reading Pace=moderate",
      "slice_dimension": "Reading Pace",
      "slice_value": "moderate",
      "count": 1140434,
      "mae": 1.735067464397787,
      "rmse": 5.421878290934029,
      "mean_predicted": -2.211018574685618,
      "mean_actual": -2.224992849827974,
      "mean_error": -0.013974275142357304,
      "std_error": 5.421862659456564
    },
    {
      "slice_name": "Reading Pace=quick_read",
      "slice_dimension": "Reading Pace",
      "slice_value": "quick_read",
      "count": 360065,
      "mae": 1.6263102048763913,
      "rmse": 2.0967446009769466,
      "mean_predicted": -2.4087749244548884,
      "mean_actual": -2.5415041626693187,
      "mean_error": -0.13272923821442623,
      "std_error": 2.0925422413897845
    },
    {
      "slice_name": "Reading Pace=very_long",
      "slice_dimension": "Reading Pace",
      "slice_value": "very_long",
      "count": 188933,
      "mae": 1.6878176628259005,
      "rmse": 4.671771148894618,
      "mean_predicted": -1.678095723302148,
      "mean_actual": -1.7540775685033128,
      "mean_error": -0.07598184520116354,
      "std_error": 4.6711655843480795
    },
    {
      "slice_name": "Author Gender=Female",
      "slice_dimension": "Author Gender",
      "slice_value": "Female",
      "count": 1140853,
      "mae": 1.7297131770978094,
      "rmse": 5.351454302017657,
      "mean_predicted": -2.1739852447551704,
      "mean_actual": -2.2138356551985643,
      "mean_error": -0.039850410443393214,
      "std_error": 5.35130826922967
    },
    {
      "slice_name": "Author Gender=Male",
      "slice_dimension": "Author Gender",
      "slice_value": "Male",
      "count": 1355558,
      "mae": 1.743573242868717,
      "rmse": 2.8185508830714907,
      "mean_predicted": -1.8657442849598858,
      "mean_actual": -1.84797482303335,
      "mean_error": 0.017769461926528448,
      "std_error": 2.818495908626381
    },
    {
      "slice_name": "Author Gender=Unknown",
      "slice_dimension": "Author Gender",
      "slice_value": "Unknown",
      "count": 362043,
      "mae": 1.7013915447123837,
      "rmse": 1.922176219305671,
      "mean_predicted": -2.263504295963209,
      "mean_actual": -2.305124857466923,
      "mean_error": -0.04162056150371189,
      "std_error": 1.9217282189422218
    },
    {
      "slice_name": "Rating Range=Low (1-3)",
      "slice_dimension": "Rating Range",
      "slice_value": "Low (1-3)",
      "count": 2858441,
      "mae": 1.7326870605544789,
      "rmse": 3.9579308701730915,
      "mean_predicted": -2.039150729831846,
      "mean_actual": -2.051920154541298,
      "mean_error": -0.012769424709466952,
      "std_error": 3.957910963518804
    },
    {
      "slice_name": "Rating Range=Medium (3-4)",
      "slice_dimension": "Rating Range",
      "slice_value": "Medium (3-4)",
      "count": 13,
      "mae": 4.33001169916833,
      "rmse": 4.369024489754259,
      "mean_predicted": -1.2318737260303576,
      "mean_actual": 3.0981379731379732,
      "mean_error": 4.33001169916833,
      "std_error": 0.6063454600302814
    }
  ],
  "disparity_analysis": {
    "summary": {
      "Popularity": {
        "mae_range": 0.42326375812838424,
        "rmse_range": 2.24145488605134,
        "mae_coefficient_of_variation": 0.11748431243327925,
        "num_slices": 3,
        "max_mae_slice": "Medium",
        "min_mae_slice": "High"
      },
      "Book Length": {
        "mae_range": 0.060774228084714776,
        "rmse_range": 3.2193898400150815,
        "mae_coefficient_of_variation": 0.012904130166108087,
        "num_slices": 4,
        "max_mae_slice": "very_long",
        "min_mae_slice": "medium"
      },
      "Book Era": {
        "mae_range": 0.779368549985332,
        "rmse_range": 6.023790091077283,
        "mae_coefficient_of_variation": 0.19986969537916568,
        "num_slices": 4,
        "max_mae_slice": "classic",
        "min_mae_slice": "recent"
      },
      "Genre Diversity": {
        "mae_range": 0.050599784696110595,
        "rmse_range": 1.9463988465883761,
        "mae_coefficient_of_variation": 0.014817764778584173,
        "num_slices": 2,
        "max_mae_slice": "Multi-genre",
        "min_mae_slice": "Some genres"
      },
      "Reading Pace": {
        "mae_range": 0.1440998105963469,
        "rmse_range": 3.3251336899570827,
        "mae_coefficient_of_variation": 0.03168126124369286,
        "num_slices": 4,
        "max_mae_slice": "long_read",
        "min_mae_slice": "quick_read"
      },
      "Author Gender": {
        "mae_range": 0.042181698156333214,
        "rmse_range": 3.4292780827119858,
        "mae_coefficient_of_variation": 0.010177279286273523,
        "num_slices": 3,
        "max_mae_slice": "Male",
        "min_mae_slice": "Unknown"
      },
      "Rating Range": {
        "mae_range": 2.597324638613851,
        "rmse_range": 0.4110936195811674,
        "mae_coefficient_of_variation": 0.42841063716855404,
        "num_slices": 2,
        "max_mae_slice": "Medium (3-4)",
        "min_mae_slice": "Low (1-3)"
      }
    },
    "detailed_disparities": [
      {
        "dimension": "Popularity",
        "severity": "medium",
        "mae_range": 0.42326375812838424,
        "mae_cv": 0.11748431243327925,
        "recommendation": "Significant performance disparity detected in Popularity. Consider bias mitigation."
      },
      {
        "dimension": "Book Era",
        "severity": "medium",
        "mae_range": 0.779368549985332,
        "mae_cv": 0.19986969537916568,
        "recommendation": "Significant performance disparity detected in Book Era. Consider bias mitigation."
      },
      {
        "dimension": "Rating Range",
        "severity": "high",
        "mae_range": 2.597324638613851,
        "mae_cv": 0.42841063716855404,
        "recommendation": "Significant performance disparity detected in Rating Range. Consider bias mitigation."
      }
    ],
    "high_risk_slices": [
      {
        "slice": "Rating Range=Medium (3-4)",
        "dimension": "Rating Range",
        "mae": 4.33001169916833,
        "mae_deviation_pct": 42.8410637168554,
        "count": 13
      }
    ]
  },
  "recommendations": [
    "HIGH PRIORITY: 1 dimensions show severe performance disparities. Dimensions: Rating Range",
    "MEDIUM PRIORITY: 2 dimensions show moderate disparities. Dimensions: Popularity, Book Era",
    "Target these high-error slices for mitigation: Rating Range=Medium (3-4) (MAE: 4.330)",
    "Rating Range: Consider re-weighting training data or adjusting decision thresholds to balance performance across 2 groups"
  ]
}