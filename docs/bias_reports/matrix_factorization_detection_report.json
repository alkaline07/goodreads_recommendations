{
  "timestamp": "2025-12-11T23:17:30.969333",
  "model_name": "matrix_factorization",
  "dataset": "test",
  "slice_metrics": [
    {
      "slice_name": "Popularity=High",
      "slice_dimension": "Popularity",
      "slice_value": "High",
      "count": 60639,
      "mae": 1.3279160116317918,
      "rmse": 1.7647843478585012,
      "mean_predicted": -0.4884428772038203,
      "mean_actual": -0.7430582907965294,
      "mean_error": -0.25461541359270945,
      "std_error": 1.7463347554455875
    },
    {
      "slice_name": "Popularity=Low",
      "slice_dimension": "Popularity",
      "slice_value": "Low",
      "count": 2779053,
      "mae": 1.755556024770924,
      "rmse": 2.045558361544031,
      "mean_predicted": -2.0980414640666716,
      "mean_actual": -2.084675699996615,
      "mean_error": 0.013365764070037948,
      "std_error": 2.0455150628704253
    },
    {
      "slice_name": "Popularity=Medium",
      "slice_dimension": "Popularity",
      "slice_value": "Medium",
      "count": 18770,
      "mae": 1.7941756957481607,
      "rmse": 1.9969395028878654,
      "mean_predicted": -1.3440880391157828,
      "mean_actual": -1.4261731242374254,
      "mean_error": -0.08208508512164292,
      "std_error": 1.9953048698454061
    },
    {
      "slice_name": "Book Length=long",
      "slice_dimension": "Book Length",
      "slice_value": "long",
      "count": 1149750,
      "mae": 1.765210806810765,
      "rmse": 2.149232718055854,
      "mean_predicted": -2.021730690235154,
      "mean_actual": -1.9878708453967544,
      "mean_error": 0.033859844838390225,
      "std_error": 2.1489669154842144
    },
    {
      "slice_name": "Book Length=medium",
      "slice_dimension": "Book Length",
      "slice_value": "medium",
      "count": 1213392,
      "mae": 1.7203589108865354,
      "rmse": 1.9450199532944004,
      "mean_predicted": -2.2170396798345364,
      "mean_actual": -2.2461330564052306,
      "mean_error": -0.02909337657067823,
      "std_error": 1.9448031548845843
    },
    {
      "slice_name": "Book Length=short",
      "slice_dimension": "Book Length",
      "slice_value": "short",
      "count": 199390,
      "mae": 1.7667666177453445,
      "rmse": 1.9993005835919753,
      "mean_predicted": -1.9381690404926153,
      "mean_actual": -1.9528527936319633,
      "mean_error": -0.014683753139351942,
      "std_error": 1.999251674282675
    },
    {
      "slice_name": "Book Length=very_long",
      "slice_dimension": "Book Length",
      "slice_value": "very_long",
      "count": 295930,
      "mae": 1.769631174618599,
      "rmse": 2.00850545115691,
      "mean_predicted": -1.6366743746437424,
      "mean_actual": -1.5709057369830977,
      "mean_error": 0.06576863766064324,
      "std_error": 2.0074317549986187
    },
    {
      "slice_name": "Book Era=classic",
      "slice_dimension": "Book Era",
      "slice_value": "classic",
      "count": 707249,
      "mae": 1.8320547009928383,
      "rmse": 2.0113438014383456,
      "mean_predicted": -1.6978081054140408,
      "mean_actual": -1.511495575296481,
      "mean_error": 0.18631253011755905,
      "std_error": 2.002697480821516
    },
    {
      "slice_name": "Book Era=contemporary",
      "slice_dimension": "Book Era",
      "slice_value": "contemporary",
      "count": 564317,
      "mae": 1.5703018011490413,
      "rmse": 2.1618424229821835,
      "mean_predicted": -2.603771533764883,
      "mean_actual": -2.8264500674531994,
      "mean_error": -0.22267853368832255,
      "std_error": 2.1503453504936374
    },
    {
      "slice_name": "Book Era=modern",
      "slice_dimension": "Book Era",
      "slice_value": "modern",
      "count": 1586858,
      "mae": 1.7714723705925257,
      "rmse": 2.0072453173500153,
      "mean_predicted": -2.0261270180448263,
      "mean_actual": -2.017248463091346,
      "mean_error": 0.008878554953482361,
      "std_error": 2.0072263136572994
    },
    {
      "slice_name": "Book Era=recent",
      "slice_dimension": "Book Era",
      "slice_value": "recent",
      "count": 38,
      "mae": 1.0909039738999844,
      "rmse": 1.366631528059805,
      "mean_predicted": -3.0039999382604137,
      "mean_actual": -3.8794960052125496,
      "mean_error": -0.8754960669521359,
      "std_error": 1.0634614323437874
    },
    {
      "slice_name": "Genre Diversity=Multi-genre",
      "slice_dimension": "Genre Diversity",
      "slice_value": "Multi-genre",
      "count": 2858249,
      "mae": 1.7467292167007515,
      "rmse": 2.0396665828620804,
      "mean_predicted": -2.0589135124381417,
      "mean_actual": -2.0518670167429818,
      "mean_error": 0.0070464956951834705,
      "std_error": 2.0396547677600325
    },
    {
      "slice_name": "Genre Diversity=Some genres",
      "slice_dimension": "Genre Diversity",
      "slice_value": "Some genres",
      "count": 213,
      "mae": 1.8609260118696047,
      "rmse": 2.296876861411312,
      "mean_predicted": -2.4796100984399243,
      "mean_actual": -2.3708478952176226,
      "mean_error": 0.10876220322230146,
      "std_error": 2.299705070531361
    },
    {
      "slice_name": "User Activity=Low",
      "slice_dimension": "User Activity",
      "slice_value": "Low",
      "count": 2858462,
      "mae": 1.746737726143024,
      "rmse": 2.0396869574041387,
      "mean_predicted": -2.0589448608950693,
      "mean_actual": -2.0518907857932827,
      "mean_error": 0.007054075101767897,
      "std_error": 2.039675116201877
    },
    {
      "slice_name": "Reading Pace=long_read",
      "slice_dimension": "Reading Pace",
      "slice_value": "long_read",
      "count": 1169322,
      "mae": 1.7868413835768677,
      "rmse": 1.978390696953675,
      "mean_predicted": -1.858632140366006,
      "mean_actual": -1.7802721345794106,
      "mean_error": 0.07836000578659919,
      "std_error": 1.9768390934271667
    },
    {
      "slice_name": "Reading Pace=moderate",
      "slice_dimension": "Reading Pace",
      "slice_value": "moderate",
      "count": 1139846,
      "mae": 1.7435048624521947,
      "rmse": 1.975708962184488,
      "mean_predicted": -2.2241434871069052,
      "mean_actual": -2.2250840515856565,
      "mean_error": -0.0009405644787581802,
      "std_error": 1.9757096049564629
    },
    {
      "slice_name": "Reading Pace=quick_read",
      "slice_dimension": "Reading Pace",
      "slice_value": "quick_read",
      "count": 360372,
      "mae": 1.661964455868256,
      "rmse": 2.4421192716626003,
      "mean_predicted": -2.393261760649032,
      "mean_actual": -2.541522070405311,
      "mean_error": -0.14826030975627646,
      "std_error": 2.437618080430988
    },
    {
      "slice_name": "Reading Pace=very_long",
      "slice_dimension": "Reading Pace",
      "slice_value": "very_long",
      "count": 188922,
      "mae": 1.6797301292730695,
      "rmse": 1.9478871828949125,
      "mean_predicted": -1.6643408862560787,
      "mean_actual": -1.754130905716518,
      "mean_error": -0.08979001946043906,
      "std_error": 1.9458217469451329
    },
    {
      "slice_name": "Author Gender=Female",
      "slice_dimension": "Author Gender",
      "slice_value": "Female",
      "count": 1140858,
      "mae": 1.7365603424475113,
      "rmse": 2.052476413579296,
      "mean_predicted": -2.1724814792547793,
      "mean_actual": -2.2138281440286685,
      "mean_error": -0.0413466647738836,
      "std_error": 2.0520608111487184
    },
    {
      "slice_name": "Author Gender=Male",
      "slice_dimension": "Author Gender",
      "slice_value": "Male",
      "count": 1355561,
      "mae": 1.7610919318444025,
      "rmse": 2.0544879062902113,
      "mean_predicted": -1.9090720936058811,
      "mean_actual": -1.8479699340923124,
      "mean_error": 0.061102159513568864,
      "std_error": 2.053579848514528
    },
    {
      "slice_name": "Author Gender=Unknown",
      "slice_dimension": "Author Gender",
      "slice_value": "Unknown",
      "count": 362043,
      "mae": 1.7250633619744535,
      "rmse": 1.941265552972788,
      "mean_predicted": -2.2623254509096333,
      "mean_actual": -2.3051193332612345,
      "mean_error": -0.04279388235160167,
      "std_error": 1.9407964949422287
    },
    {
      "slice_name": "Rating Range=Low (1-3)",
      "slice_dimension": "Rating Range",
      "slice_value": "Low (1-3)",
      "count": 2858450,
      "mae": 1.7467268245619527,
      "rmse": 2.0396713387432506,
      "mean_predicted": -2.058948310546045,
      "mean_actual": -2.0519124403554025,
      "mean_error": 0.007035870190639359,
      "std_error": 2.0396595603260317
    },
    {
      "slice_name": "Rating Range=Medium (3-4)",
      "slice_dimension": "Rating Range",
      "slice_value": "Medium (3-4)",
      "count": 12,
      "mae": 4.34353975972642,
      "rmse": 4.397418593241382,
      "mean_predicted": -1.2372236221602824,
      "mean_actual": 3.1063161375661377,
      "mean_error": 4.34353975972642,
      "std_error": 0.7167750806297254
    }
  ],
  "disparity_analysis": {
    "summary": {
      "Popularity": {
        "mae_range": 0.46625968411636887,
        "rmse_range": 0.2807740136855297,
        "mae_coefficient_of_variation": 0.12994989021427183,
        "num_slices": 3,
        "max_mae_slice": "Medium",
        "min_mae_slice": "High"
      },
      "Book Length": {
        "mae_range": 0.049272263732063726,
        "rmse_range": 0.20421276476145356,
        "mae_coefficient_of_variation": 0.011589856516520733,
        "num_slices": 4,
        "max_mae_slice": "very_long",
        "min_mae_slice": "medium"
      },
      "Book Era": {
        "mae_range": 0.7411507270928539,
        "rmse_range": 0.7952108949223786,
        "mae_coefficient_of_variation": 0.1858060226066071,
        "num_slices": 4,
        "max_mae_slice": "classic",
        "min_mae_slice": "recent"
      },
      "Genre Diversity": {
        "mae_range": 0.11419679516885317,
        "rmse_range": 0.25721027854923184,
        "mae_coefficient_of_variation": 0.0316540212225621,
        "num_slices": 2,
        "max_mae_slice": "Some genres",
        "min_mae_slice": "Multi-genre"
      },
      "Reading Pace": {
        "mae_range": 0.1248769277086117,
        "rmse_range": 0.49423208876768787,
        "mae_coefficient_of_variation": 0.029094993097288992,
        "num_slices": 4,
        "max_mae_slice": "long_read",
        "min_mae_slice": "quick_read"
      },
      "Author Gender": {
        "mae_range": 0.036028569869948956,
        "rmse_range": 0.11322235331742325,
        "mae_coefficient_of_variation": 0.008631165734801924,
        "num_slices": 3,
        "max_mae_slice": "Male",
        "min_mae_slice": "Unknown"
      },
      "Rating Range": {
        "mae_range": 2.5968129351644675,
        "rmse_range": 2.3577472544981317,
        "mae_coefficient_of_variation": 0.4263874001613833,
        "num_slices": 2,
        "max_mae_slice": "Medium (3-4)",
        "min_mae_slice": "Low (1-3)"
      }
    },
    "detailed_disparities": [
      {
        "dimension": "Popularity",
        "severity": "medium",
        "mae_range": 0.46625968411636887,
        "mae_cv": 0.12994989021427183,
        "recommendation": "Significant performance disparity detected in Popularity. Consider bias mitigation."
      },
      {
        "dimension": "Book Era",
        "severity": "medium",
        "mae_range": 0.7411507270928539,
        "mae_cv": 0.1858060226066071,
        "recommendation": "Significant performance disparity detected in Book Era. Consider bias mitigation."
      },
      {
        "dimension": "Rating Range",
        "severity": "high",
        "mae_range": 2.5968129351644675,
        "mae_cv": 0.4263874001613833,
        "recommendation": "Significant performance disparity detected in Rating Range. Consider bias mitigation."
      }
    ],
    "high_risk_slices": [
      {
        "slice": "Rating Range=Medium (3-4)",
        "dimension": "Rating Range",
        "mae": 4.34353975972642,
        "mae_deviation_pct": 42.638740016138335,
        "count": 12
      }
    ]
  },
  "recommendations": [
    "HIGH PRIORITY: 1 dimensions show severe performance disparities. Dimensions: Rating Range",
    "MEDIUM PRIORITY: 2 dimensions show moderate disparities. Dimensions: Popularity, Book Era",
    "Target these high-error slices for mitigation: Rating Range=Medium (3-4) (MAE: 4.344)",
    "Rating Range: Consider re-weighting training data or adjusting decision thresholds to balance performance across 2 groups"
  ]
}