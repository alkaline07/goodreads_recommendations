{
  "timestamp": "2025-12-10T04:16:52.064465",
  "model_name": "matrix_factorization",
  "dataset": "test",
  "slice_metrics": [
    {
      "slice_name": "Popularity=High",
      "slice_dimension": "Popularity",
      "slice_value": "High",
      "count": 60639,
      "mae": 1.4358396576840047,
      "rmse": 1.7489502791210616,
      "mean_predicted": -0.8526281453987548,
      "mean_actual": -0.7430582907965289,
      "mean_error": 0.10956985460222518,
      "std_error": 1.7455290807877077
    },
    {
      "slice_name": "Popularity=Low",
      "slice_dimension": "Popularity",
      "slice_value": "Low",
      "count": 2779045,
      "mae": 1.7424218075258702,
      "rmse": 10.897461173496627,
      "mean_predicted": -2.093189135323649,
      "mean_actual": -2.0846819110291603,
      "mean_error": 0.00850722429448824,
      "std_error": 10.897459813514667
    },
    {
      "slice_name": "Popularity=Medium",
      "slice_dimension": "Popularity",
      "slice_value": "Medium",
      "count": 18770,
      "mae": 1.7449817927282663,
      "rmse": 2.02610604043095,
      "mean_predicted": -1.0739285074209666,
      "mean_actual": -1.4261731242374258,
      "mean_error": -0.352244616816459,
      "std_error": 1.9953048698454066
    },
    {
      "slice_name": "Book Length=long",
      "slice_dimension": "Book Length",
      "slice_value": "long",
      "count": 1149748,
      "mae": 1.7651558939932217,
      "rmse": 14.932788917463899,
      "mean_predicted": -2.020278565696383,
      "mean_actual": -1.987876477710705,
      "mean_error": 0.03240208798569108,
      "std_error": 14.932760257325333
    },
    {
      "slice_name": "Book Length=medium",
      "slice_dimension": "Book Length",
      "slice_value": "medium",
      "count": 1213389,
      "mae": 1.7011126900234805,
      "rmse": 7.700160142771768,
      "mean_predicted": -2.2297445279087476,
      "mean_actual": -2.246139021845137,
      "mean_error": -0.01639449393638919,
      "std_error": 7.700145862897286
    },
    {
      "slice_name": "Book Length=short",
      "slice_dimension": "Book Length",
      "slice_value": "short",
      "count": 199390,
      "mae": 1.7431884712031782,
      "rmse": 1.9726564660409727,
      "mean_predicted": -1.9038132474889398,
      "mean_actual": -1.9528540474561256,
      "mean_error": -0.049040799967187576,
      "std_error": 1.9720517329371803
    },
    {
      "slice_name": "Book Length=very_long",
      "slice_dimension": "Book Length",
      "slice_value": "very_long",
      "count": 295927,
      "mae": 1.760297530919883,
      "rmse": 2.002048779558315,
      "mean_predicted": -1.6252890409623402,
      "mean_actual": -1.5709126510211697,
      "mean_error": 0.05437638994117385,
      "std_error": 2.0013135832638294
    },
    {
      "slice_name": "Book Era=classic",
      "slice_dimension": "Book Era",
      "slice_value": "classic",
      "count": 707249,
      "mae": 1.835665740233287,
      "rmse": 9.967934675937949,
      "mean_predicted": -1.6495985426407533,
      "mean_actual": -1.511500877530915,
      "mean_error": 0.1380976651098366,
      "std_error": 9.966985060664062
    },
    {
      "slice_name": "Book Era=contemporary",
      "slice_dimension": "Book Era",
      "slice_value": "contemporary",
      "count": 564316,
      "mae": 1.5164409397796985,
      "rmse": 1.8393193487780004,
      "mean_predicted": -2.6604584200565067,
      "mean_actual": -2.826457291154224,
      "mean_error": -0.16599887109771266,
      "std_error": 1.831814943656009
    },
    {
      "slice_name": "Book Era=modern",
      "slice_dimension": "Book Era",
      "slice_value": "modern",
      "count": 1586851,
      "mae": 1.7695591835070807,
      "rmse": 12.753542214587595,
      "mean_predicted": -2.029675921572326,
      "mean_actual": -2.017254578393022,
      "mean_error": 0.012421343179312812,
      "std_error": 12.753540184194122
    },
    {
      "slice_name": "Book Era=recent",
      "slice_dimension": "Book Era",
      "slice_value": "recent",
      "count": 38,
      "mae": 1.0104461361665158,
      "rmse": 1.2788355901446682,
      "mean_predicted": -3.092565498095618,
      "mean_actual": -3.8794960052125496,
      "mean_error": -0.786930507116931,
      "std_error": 1.0215794879366205
    },
    {
      "slice_name": "Genre Diversity=Multi-genre",
      "slice_dimension": "Genre Diversity",
      "slice_value": "Multi-genre",
      "count": 2858241,
      "mae": 1.7359309562457468,
      "rmse": 10.749685171290963,
      "mean_predicted": -2.0601480262819636,
      "mean_actual": -2.051872963851505,
      "mean_error": 0.008275062430452982,
      "std_error": 10.749683866708505
    },
    {
      "slice_name": "Genre Diversity=Some genres",
      "slice_dimension": "Genre Diversity",
      "slice_value": "Some genres",
      "count": 213,
      "mae": 1.7876480598891789,
      "rmse": 2.1144387129554985,
      "mean_predicted": -2.4760656359395288,
      "mean_actual": -2.370847895217623,
      "mean_error": 0.10521774072190632,
      "std_error": 2.1167940375484324
    },
    {
      "slice_name": "User Activity=Low",
      "slice_dimension": "User Activity",
      "slice_value": "Low",
      "count": 2858454,
      "mae": 1.7359348099873368,
      "rmse": 10.749300149434779,
      "mean_predicted": -2.0601790187173314,
      "mean_actual": -2.051896732525196,
      "mean_error": 0.008282286192135866,
      "std_error": 10.74929883896696
    },
    {
      "slice_name": "Reading Pace=long_read",
      "slice_dimension": "Reading Pace",
      "slice_value": "long_read",
      "count": 1169311,
      "mae": 1.7708703920667952,
      "rmse": 1.9732780119886333,
      "mean_predicted": -1.8207669344447226,
      "mean_actual": -1.780528384960683,
      "mean_error": 0.04023854948404335,
      "std_error": 1.9728685461451594
    },
    {
      "slice_name": "Reading Pace=moderate",
      "slice_dimension": "Reading Pace",
      "slice_value": "moderate",
      "count": 1140303,
      "mae": 1.747833463714236,
      "rmse": 16.83371155577567,
      "mean_predicted": -2.2436654206687607,
      "mean_actual": -2.224928118657431,
      "mean_error": 0.018737302011317514,
      "std_error": 16.83370850894053
    },
    {
      "slice_name": "Reading Pace=quick_read",
      "slice_dimension": "Reading Pace",
      "slice_value": "quick_read",
      "count": 359918,
      "mae": 1.6105750475133187,
      "rmse": 2.2934600379782344,
      "mean_predicted": -2.421364717501132,
      "mean_actual": -2.5416623199039465,
      "mean_error": -0.12029760240281062,
      "std_error": 2.2903060945685443
    },
    {
      "slice_name": "Reading Pace=very_long",
      "slice_dimension": "Reading Pace",
      "slice_value": "very_long",
      "count": 188922,
      "mae": 1.6867113422729623,
      "rmse": 1.9369437706575496,
      "mean_predicted": -1.7463979595162866,
      "mean_actual": -1.7540507139571566,
      "mean_error": -0.007652754440868726,
      "std_error": 1.936933779086067
    },
    {
      "slice_name": "Author Gender=Female",
      "slice_dimension": "Author Gender",
      "slice_value": "Female",
      "count": 1140853,
      "mae": 1.741131623481438,
      "rmse": 16.79686238312357,
      "mean_predicted": -2.2176941692654792,
      "mean_actual": -2.213835655198568,
      "mean_error": 0.0038585140669243965,
      "std_error": 16.796869301482886
    },
    {
      "slice_name": "Author Gender=Male",
      "slice_dimension": "Author Gender",
      "slice_value": "Male",
      "count": 1355558,
      "mae": 1.7441252246624426,
      "rmse": 2.285620880128963,
      "mean_predicted": -1.8728522544994715,
      "mean_actual": -1.8479748230333457,
      "mean_error": 0.024877431466126095,
      "std_error": 2.2854863321493015
    },
    {
      "slice_name": "Author Gender=Unknown",
      "slice_dimension": "Author Gender",
      "slice_value": "Unknown",
      "count": 362043,
      "mae": 1.6888923802170202,
      "rmse": 1.9177235348277184,
      "mean_predicted": -2.26521174080096,
      "mean_actual": -2.305124857466927,
      "mean_error": -0.03991311666596619,
      "std_error": 1.9173107867174144
    },
    {
      "slice_name": "Rating Range=Low (1-3)",
      "slice_dimension": "Rating Range",
      "slice_value": "Low (1-3)",
      "count": 2858441,
      "mae": 1.7359224007476521,
      "rmse": 10.749320311953985,
      "mean_predicted": -2.060182174244721,
      "mean_actual": -2.0519201545413135,
      "mean_error": 0.008262019703409057,
      "std_error": 10.749319017100355
    },
    {
      "slice_name": "Rating Range=Medium (3-4)",
      "slice_dimension": "Rating Range",
      "slice_value": "Medium (3-4)",
      "count": 13,
      "mae": 4.464479386386365,
      "rmse": 4.4985375114835024,
      "mean_predicted": -1.3663414132483929,
      "mean_actual": 3.0981379731379732,
      "mean_error": 4.464479386386365,
      "std_error": 0.5750671091142442
    }
  ],
  "disparity_analysis": {
    "summary": {
      "Popularity": {
        "mae_range": 0.3091421350442616,
        "rmse_range": 9.148510894375566,
        "mae_coefficient_of_variation": 0.08843643985711565,
        "num_slices": 3,
        "max_mae_slice": "Medium",
        "min_mae_slice": "High"
      },
      "Book Length": {
        "mae_range": 0.06404320396974117,
        "rmse_range": 12.960132451422925,
        "mae_coefficient_of_variation": 0.01447173818112423,
        "num_slices": 4,
        "max_mae_slice": "long",
        "min_mae_slice": "medium"
      },
      "Book Era": {
        "mae_range": 0.8252196040667712,
        "rmse_range": 11.474706624442927,
        "mae_coefficient_of_variation": 0.21159782786955214,
        "num_slices": 4,
        "max_mae_slice": "classic",
        "min_mae_slice": "recent"
      },
      "Genre Diversity": {
        "mae_range": 0.05171710364343207,
        "rmse_range": 8.635246458335464,
        "mae_coefficient_of_variation": 0.014677435473026926,
        "num_slices": 2,
        "max_mae_slice": "Some genres",
        "min_mae_slice": "Multi-genre"
      },
      "Reading Pace": {
        "mae_range": 0.1602953445534765,
        "rmse_range": 14.896767785118119,
        "mae_coefficient_of_variation": 0.036437218077508556,
        "num_slices": 4,
        "max_mae_slice": "long_read",
        "min_mae_slice": "quick_read"
      },
      "Author Gender": {
        "mae_range": 0.055232844445422424,
        "rmse_range": 14.879138848295852,
        "mae_coefficient_of_variation": 0.014704375784308494,
        "num_slices": 3,
        "max_mae_slice": "Male",
        "min_mae_slice": "Unknown"
      },
      "Rating Range": {
        "mae_range": 2.728556985638713,
        "rmse_range": 6.250782800470483,
        "mae_coefficient_of_variation": 0.44006131849399416,
        "num_slices": 2,
        "max_mae_slice": "Medium (3-4)",
        "min_mae_slice": "Low (1-3)"
      }
    },
    "detailed_disparities": [
      {
        "dimension": "Popularity",
        "severity": "medium",
        "mae_range": 0.3091421350442616,
        "mae_cv": 0.08843643985711565,
        "recommendation": "Significant performance disparity detected in Popularity. Consider bias mitigation."
      },
      {
        "dimension": "Book Era",
        "severity": "medium",
        "mae_range": 0.8252196040667712,
        "mae_cv": 0.21159782786955214,
        "recommendation": "Significant performance disparity detected in Book Era. Consider bias mitigation."
      },
      {
        "dimension": "Rating Range",
        "severity": "high",
        "mae_range": 2.728556985638713,
        "mae_cv": 0.44006131849399416,
        "recommendation": "Significant performance disparity detected in Rating Range. Consider bias mitigation."
      }
    ],
    "high_risk_slices": [
      {
        "slice": "Rating Range=Medium (3-4)",
        "dimension": "Rating Range",
        "mae": 4.464479386386365,
        "mae_deviation_pct": 44.00613184939941,
        "count": 13
      }
    ]
  },
  "recommendations": [
    "HIGH PRIORITY: 1 dimensions show severe performance disparities. Dimensions: Rating Range",
    "MEDIUM PRIORITY: 2 dimensions show moderate disparities. Dimensions: Popularity, Book Era",
    "Target these high-error slices for mitigation: Rating Range=Medium (3-4) (MAE: 4.464)",
    "Book Era: Consider re-weighting training data or adjusting decision thresholds to balance performance across 4 groups",
    "Rating Range: Consider re-weighting training data or adjusting decision thresholds to balance performance across 2 groups"
  ]
}