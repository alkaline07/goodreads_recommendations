{
  "timestamp": "2025-12-11T23:06:58.926282",
  "model_name": "boosted_tree_regressor",
  "dataset": "test",
  "slice_metrics": [
    {
      "slice_name": "Popularity=High",
      "slice_dimension": "Popularity",
      "slice_value": "High",
      "count": 60639,
      "mae": 0.6144678523120666,
      "rmse": 0.9079530506050972,
      "mean_predicted": -0.7442110812477705,
      "mean_actual": -0.7430582907965295,
      "mean_error": 0.0011527904512404913,
      "std_error": 0.9079598054099075
    },
    {
      "slice_name": "Popularity=Low",
      "slice_dimension": "Popularity",
      "slice_value": "Low",
      "count": 2779053,
      "mae": 0.4484048615340521,
      "rmse": 0.827509903188617,
      "mean_predicted": -2.031342419653999,
      "mean_actual": -2.084675699996624,
      "mean_error": -0.0533332803426204,
      "std_error": 0.8257895897047378
    },
    {
      "slice_name": "Popularity=Medium",
      "slice_dimension": "Popularity",
      "slice_value": "Medium",
      "count": 18770,
      "mae": 0.6297519935550413,
      "rmse": 1.0136424796225936,
      "mean_predicted": -1.4095980645171993,
      "mean_actual": -1.4261731242374263,
      "mean_error": -0.016575059720226926,
      "std_error": 1.0135339521879174
    },
    {
      "slice_name": "Book Length=long",
      "slice_dimension": "Book Length",
      "slice_value": "long",
      "count": 1149750,
      "mae": 0.4671435625058518,
      "rmse": 0.8485654205869366,
      "mean_predicted": -1.9339343504234896,
      "mean_actual": -1.987870845396752,
      "mean_error": -0.05393649497326431,
      "std_error": 0.84684989890435
    },
    {
      "slice_name": "Book Length=medium",
      "slice_dimension": "Book Length",
      "slice_value": "medium",
      "count": 1213392,
      "mae": 0.4203126141146683,
      "rmse": 0.7945970198925869,
      "mean_predicted": -2.192836254600282,
      "mean_actual": -2.24613305640522,
      "mean_error": -0.05329680180494273,
      "std_error": 0.7928079168030513
    },
    {
      "slice_name": "Book Length=short",
      "slice_dimension": "Book Length",
      "slice_value": "short",
      "count": 199390,
      "mae": 0.46207484037382107,
      "rmse": 0.8325647361217594,
      "mean_predicted": -1.910922617325388,
      "mean_actual": -1.9528527936319602,
      "mean_error": -0.041930176306577284,
      "std_error": 0.8315102932444572
    },
    {
      "slice_name": "Book Length=very_long",
      "slice_dimension": "Book Length",
      "slice_value": "very_long",
      "count": 295930,
      "mae": 0.5271066123647712,
      "rmse": 0.9006976400129438,
      "mean_predicted": -1.5255795900503992,
      "mean_actual": -1.5709057369831,
      "mean_error": -0.04532614693270076,
      "std_error": 0.8995579545392356
    },
    {
      "slice_name": "Book Era=classic",
      "slice_dimension": "Book Era",
      "slice_value": "classic",
      "count": 707249,
      "mae": 0.4819993040009772,
      "rmse": 0.8231337379760111,
      "mean_predicted": -1.4751981219674024,
      "mean_actual": -1.5114955752964836,
      "mean_error": -0.03629745332907818,
      "std_error": 0.8223336315761332
    },
    {
      "slice_name": "Book Era=contemporary",
      "slice_dimension": "Book Era",
      "slice_value": "contemporary",
      "count": 564317,
      "mae": 0.3783453931774132,
      "rmse": 0.7836171222457821,
      "mean_predicted": -2.7574942779398546,
      "mean_actual": -2.826450067453203,
      "mean_error": -0.06895578951333688,
      "std_error": 0.7805779737381396
    },
    {
      "slice_name": "Book Era=modern",
      "slice_dimension": "Book Era",
      "slice_value": "modern",
      "count": 1586858,
      "mae": 0.466841425348694,
      "rmse": 0.8500182068661111,
      "mean_predicted": -1.9643989140862645,
      "mean_actual": -2.017248463091351,
      "mean_error": -0.052849549005066165,
      "std_error": 0.8483739333193475
    },
    {
      "slice_name": "Book Era=recent",
      "slice_dimension": "Book Era",
      "slice_value": "recent",
      "count": 38,
      "mae": 0.2821877133510032,
      "rmse": 0.7693043478849023,
      "mean_predicted": -3.66288886415331,
      "mean_actual": -3.879496005212549,
      "mean_error": -0.21660714105923964,
      "std_error": 0.7480894583092497
    },
    {
      "slice_name": "Genre Diversity=Multi-genre",
      "slice_dimension": "Genre Diversity",
      "slice_value": "Multi-genre",
      "count": 2858249,
      "mae": 0.45311903264244113,
      "rmse": 0.8306504034655172,
      "mean_predicted": -1.9999310397994534,
      "mean_actual": -2.0518670167429676,
      "mean_error": -0.05193597694352571,
      "std_error": 0.8290253238184606
    },
    {
      "slice_name": "Genre Diversity=Some genres",
      "slice_dimension": "Genre Diversity",
      "slice_value": "Some genres",
      "count": 213,
      "mae": 0.44607861477980065,
      "rmse": 0.8658454317934045,
      "mean_predicted": -2.3179446843194778,
      "mean_actual": -2.370847895217623,
      "mean_error": -0.052903210898144745,
      "std_error": 0.8662636007625121
    },
    {
      "slice_name": "User Activity=Low",
      "slice_dimension": "User Activity",
      "slice_value": "Low",
      "count": 2858462,
      "mae": 0.45311850802150666,
      "rmse": 0.8306530815996301,
      "mean_predicted": -1.999954736775744,
      "mean_actual": -2.051890785793289,
      "mean_error": -0.051936049017540266,
      "std_error": 0.8290280026772295
    },
    {
      "slice_name": "Reading Pace=long_read",
      "slice_dimension": "Reading Pace",
      "slice_value": "long_read",
      "count": 1169322,
      "mae": 0.49341254763184883,
      "rmse": 0.8655092203840045,
      "mean_predicted": -1.7337190351985443,
      "mean_actual": -1.7802721345794097,
      "mean_error": -0.04655309938086693,
      "std_error": 0.8642567085582237
    },
    {
      "slice_name": "Reading Pace=moderate",
      "slice_dimension": "Reading Pace",
      "slice_value": "moderate",
      "count": 1139846,
      "mae": 0.4271377692457588,
      "rmse": 0.8092829173649011,
      "mean_predicted": -2.1690922090389275,
      "mean_actual": -2.2250840515856662,
      "mean_error": -0.05599184254674107,
      "std_error": 0.8073439946777904
    },
    {
      "slice_name": "Reading Pace=quick_read",
      "slice_dimension": "Reading Pace",
      "slice_value": "quick_read",
      "count": 360372,
      "mae": 0.37467207899589067,
      "rmse": 0.7420900970436833,
      "mean_predicted": -2.485857314211668,
      "mean_actual": -2.541522070405309,
      "mean_error": -0.05566475619363878,
      "std_error": 0.7400004503991705
    },
    {
      "slice_name": "Reading Pace=very_long",
      "slice_dimension": "Reading Pace",
      "slice_value": "very_long",
      "count": 188922,
      "mae": 0.5101115035760403,
      "rmse": 0.8950932117226689,
      "mean_predicted": -1.7004602797756774,
      "mean_actual": -1.7541309057165209,
      "mean_error": -0.053670625940842466,
      "std_error": 0.8934850570764007
    },
    {
      "slice_name": "Author Gender=Female",
      "slice_dimension": "Author Gender",
      "slice_value": "Female",
      "count": 1140858,
      "mae": 0.43356614462229703,
      "rmse": 0.8187674733013152,
      "mean_predicted": -2.155327507472627,
      "mean_actual": -2.2138281440286605,
      "mean_error": -0.058500636556034394,
      "std_error": 0.8166752325557688
    },
    {
      "slice_name": "Author Gender=Male",
      "slice_dimension": "Author Gender",
      "slice_value": "Male",
      "count": 1355561,
      "mae": 0.4784408686473839,
      "rmse": 0.8472435150882208,
      "mean_predicted": -1.8030229071132293,
      "mean_actual": -1.8479699340923192,
      "mean_error": -0.04494702697908582,
      "std_error": 0.8460507471028096
    },
    {
      "slice_name": "Author Gender=Unknown",
      "slice_dimension": "Author Gender",
      "slice_value": "Unknown",
      "count": 362043,
      "mae": 0.4199193181753274,
      "rmse": 0.8046948450521934,
      "mean_predicted": -2.247701108111362,
      "mean_actual": -2.3051193332612345,
      "mean_error": -0.05741822514987412,
      "std_error": 0.8026448283816534
    },
    {
      "slice_name": "Rating Range=Low (1-3)",
      "slice_dimension": "Rating Range",
      "slice_value": "Low (1-3)",
      "count": 2858450,
      "mae": 0.4531071978066818,
      "rmse": 0.830629731219574,
      "mean_predicted": -1.9999629608639218,
      "mean_actual": -2.051912440355406,
      "mean_error": -0.051949479491485616,
      "std_error": 0.8290037650053241
    },
    {
      "slice_name": "Rating Range=Medium (3-4)",
      "slice_dimension": "Rating Range",
      "slice_value": "Medium (3-4)",
      "count": 12,
      "mae": 3.1472588050839487,
      "rmse": 3.151238127227103,
      "mean_predicted": -0.04094266751781106,
      "mean_actual": 3.1063161375661377,
      "mean_error": 3.1472588050839487,
      "std_error": 0.16535498445796656
    }
  ],
  "disparity_analysis": {
    "summary": {
      "Popularity": {
        "mae_range": 0.18134713202098918,
        "rmse_range": 0.18613257643397663,
        "mae_coefficient_of_variation": 0.14555395991961403,
        "num_slices": 3,
        "max_mae_slice": "Medium",
        "min_mae_slice": "Low"
      },
      "Book Length": {
        "mae_range": 0.10679399825010288,
        "rmse_range": 0.10610062012035693,
        "mae_coefficient_of_variation": 0.08115102648826074,
        "num_slices": 4,
        "max_mae_slice": "very_long",
        "min_mae_slice": "medium"
      },
      "Book Era": {
        "mae_range": 0.199811590649974,
        "rmse_range": 0.08071385898120875,
        "mae_coefficient_of_variation": 0.19851776097628457,
        "num_slices": 4,
        "max_mae_slice": "classic",
        "min_mae_slice": "recent"
      },
      "Genre Diversity": {
        "mae_range": 0.00704041786264048,
        "rmse_range": 0.035195028327887234,
        "mae_coefficient_of_variation": 0.00782966668431959,
        "num_slices": 2,
        "max_mae_slice": "Multi-genre",
        "min_mae_slice": "Some genres"
      },
      "Reading Pace": {
        "mae_range": 0.13543942458014963,
        "rmse_range": 0.15300311467898553,
        "mae_coefficient_of_variation": 0.11976800863158067,
        "num_slices": 4,
        "max_mae_slice": "very_long",
        "min_mae_slice": "quick_read"
      },
      "Author Gender": {
        "mae_range": 0.05852155047205648,
        "rmse_range": 0.04254867003602747,
        "mae_coefficient_of_variation": 0.0563081622441832,
        "num_slices": 3,
        "max_mae_slice": "Male",
        "min_mae_slice": "Unknown"
      },
      "Rating Range": {
        "mae_range": 2.694151607277267,
        "rmse_range": 2.320608396007529,
        "mae_coefficient_of_variation": 0.7482993687625674,
        "num_slices": 2,
        "max_mae_slice": "Medium (3-4)",
        "min_mae_slice": "Low (1-3)"
      }
    },
    "detailed_disparities": [
      {
        "dimension": "Book Era",
        "severity": "medium",
        "mae_range": 0.199811590649974,
        "mae_cv": 0.19851776097628457,
        "recommendation": "Significant performance disparity detected in Book Era. Consider bias mitigation."
      },
      {
        "dimension": "Rating Range",
        "severity": "high",
        "mae_range": 2.694151607277267,
        "mae_cv": 0.7482993687625674,
        "recommendation": "Significant performance disparity detected in Rating Range. Consider bias mitigation."
      }
    ],
    "high_risk_slices": [
      {
        "slice": "Rating Range=Medium (3-4)",
        "dimension": "Rating Range",
        "mae": 3.1472588050839487,
        "mae_deviation_pct": 74.82993687625674,
        "count": 12
      }
    ]
  },
  "recommendations": [
    "HIGH PRIORITY: 1 dimensions show severe performance disparities. Dimensions: Rating Range",
    "MEDIUM PRIORITY: 1 dimensions show moderate disparities. Dimensions: Book Era",
    "Target these high-error slices for mitigation: Rating Range=Medium (3-4) (MAE: 3.147)",
    "Rating Range: Consider re-weighting training data or adjusting decision thresholds to balance performance across 2 groups"
  ]
}