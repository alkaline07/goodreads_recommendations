{
  "timestamp": "2025-12-08T15:04:12.091492",
  "model_name": "boosted_tree_regressor",
  "dataset": "test",
  "slice_metrics": [
    {
      "slice_name": "Popularity=High",
      "slice_dimension": "Popularity",
      "slice_value": "High",
      "count": 60639,
      "mae": 0.6168353114195847,
      "rmse": 0.9086161905659391,
      "mean_predicted": -0.7496173735232226,
      "mean_actual": -0.74305829079653,
      "mean_error": 0.006559082726693274,
      "std_error": 0.9086000079427589
    },
    {
      "slice_name": "Popularity=Low",
      "slice_dimension": "Popularity",
      "slice_value": "Low",
      "count": 2779072,
      "mae": 0.4487450162173956,
      "rmse": 0.8284113443337404,
      "mean_predicted": -2.0320393355818487,
      "mean_actual": -2.084693922324684,
      "mean_error": -0.052654586742844046,
      "std_error": 0.8267364125637034
    },
    {
      "slice_name": "Popularity=Medium",
      "slice_dimension": "Popularity",
      "slice_value": "Medium",
      "count": 18770,
      "mae": 0.6300173027581804,
      "rmse": 1.014408463445301,
      "mean_predicted": -1.4146724818225938,
      "mean_actual": -1.4261731242374251,
      "mean_error": -0.011500642414831723,
      "std_error": 1.0143702897059166
    },
    {
      "slice_name": "Book Length=long",
      "slice_dimension": "Book Length",
      "slice_value": "long",
      "count": 1149762,
      "mae": 0.4673806625184267,
      "rmse": 0.849388894400744,
      "mean_predicted": -1.934503015910776,
      "mean_actual": -1.9878922807458614,
      "mean_error": -0.053389264835081225,
      "std_error": 0.8477096822266332
    },
    {
      "slice_name": "Book Length=medium",
      "slice_dimension": "Book Length",
      "slice_value": "medium",
      "count": 1213394,
      "mae": 0.4206739299709243,
      "rmse": 0.7954977774407405,
      "mean_predicted": -2.193481031693513,
      "mean_actual": -2.246143227105928,
      "mean_error": -0.05266219541242147,
      "std_error": 0.7937530638230704
    },
    {
      "slice_name": "Book Length=short",
      "slice_dimension": "Book Length",
      "slice_value": "short",
      "count": 199390,
      "mae": 0.46321220881187586,
      "rmse": 0.8342696333459688,
      "mean_predicted": -1.9138751240478153,
      "mean_actual": -1.9528540474561304,
      "mean_error": -0.03897892340831162,
      "std_error": 0.8333606348551972
    },
    {
      "slice_name": "Book Length=very_long",
      "slice_dimension": "Book Length",
      "slice_value": "very_long",
      "count": 295935,
      "mae": 0.527632016269113,
      "rmse": 0.9013304095024338,
      "mean_predicted": -1.5267228623743692,
      "mean_actual": -1.5709625472217723,
      "mean_error": -0.044239684847402067,
      "std_error": 0.9002455753625557
    },
    {
      "slice_name": "Book Era=classic",
      "slice_dimension": "Book Era",
      "slice_value": "classic",
      "count": 707249,
      "mae": 0.4822783590527065,
      "rmse": 0.8237749691657646,
      "mean_predicted": -1.4758177507919388,
      "mean_actual": -1.5115008775309153,
      "mean_error": -0.035683126738974585,
      "std_error": 0.8230023523604894
    },
    {
      "slice_name": "Book Era=contemporary",
      "slice_dimension": "Book Era",
      "slice_value": "contemporary",
      "count": 564320,
      "mae": 0.3785929594027657,
      "rmse": 0.7851965924066262,
      "mean_predicted": -2.756759937753449,
      "mean_actual": -2.8264641327290403,
      "mean_error": -0.06970419497559546,
      "std_error": 0.782097243216521
    },
    {
      "slice_name": "Book Era=modern",
      "slice_dimension": "Book Era",
      "slice_value": "modern",
      "count": 1586874,
      "mae": 0.46731819486637194,
      "rmse": 0.850797698297875,
      "mean_predicted": -1.9658704512428267,
      "mean_actual": -2.017272288144831,
      "mean_error": -0.051401836902008925,
      "std_error": 0.8492437983759181
    },
    {
      "slice_name": "Book Era=recent",
      "slice_dimension": "Book Era",
      "slice_value": "recent",
      "count": 38,
      "mae": 0.28557260622878267,
      "rmse": 0.7884131396284453,
      "mean_predicted": -3.657049015948647,
      "mean_actual": -3.8794960052125496,
      "mean_error": -0.2224469892639026,
      "std_error": 0.7665344863953625
    },
    {
      "slice_name": "Genre Diversity=Multi-genre",
      "slice_dimension": "Genre Diversity",
      "slice_value": "Multi-genre",
      "count": 2858268,
      "mae": 0.45350185231469087,
      "rmse": 0.8315449568621448,
      "mean_predicted": -2.0007575421833756,
      "mean_actual": -2.0518849522642784,
      "mean_error": -0.05112741008093348,
      "std_error": 0.829971833392559
    },
    {
      "slice_name": "Genre Diversity=Some genres",
      "slice_dimension": "Genre Diversity",
      "slice_value": "Some genres",
      "count": 213,
      "mae": 0.4440306077165019,
      "rmse": 0.8670605175379424,
      "mean_predicted": -2.3089776038467673,
      "mean_actual": -2.3708478952176226,
      "mean_error": -0.061870291370855326,
      "std_error": 0.8668876222824853
    },
    {
      "slice_name": "User Activity=Low",
      "slice_dimension": "User Activity",
      "slice_value": "Low",
      "count": 2858481,
      "mae": 0.45350114656394536,
      "rmse": 0.8315476598186188,
      "mean_predicted": -2.0007805092323534,
      "mean_actual": -2.0519087198201578,
      "mean_error": -0.0511282105878162,
      "std_error": 0.8299744921497441
    },
    {
      "slice_name": "Reading Pace=long_read",
      "slice_dimension": "Reading Pace",
      "slice_value": "long_read",
      "count": 1169207,
      "mae": 0.49373402193732036,
      "rmse": 0.8662024873670805,
      "mean_predicted": -1.7353645244815519,
      "mean_actual": -1.7806488417650281,
      "mean_error": -0.045284317283478405,
      "std_error": 0.8650183348915217
    },
    {
      "slice_name": "Reading Pace=moderate",
      "slice_dimension": "Reading Pace",
      "slice_value": "moderate",
      "count": 1140260,
      "mae": 0.4275006470775435,
      "rmse": 0.8104971108185404,
      "mean_predicted": -2.1691843457424467,
      "mean_actual": -2.224932038390566,
      "mean_error": -0.05574769264812798,
      "std_error": 0.8085779707524526
    },
    {
      "slice_name": "Reading Pace=quick_read",
      "slice_dimension": "Reading Pace",
      "slice_value": "quick_read",
      "count": 360041,
      "mae": 0.3754675476643072,
      "rmse": 0.7433179711818559,
      "mean_predicted": -2.486563377657498,
      "mean_actual": -2.540998408877375,
      "mean_error": -0.054435031219878395,
      "std_error": 0.7413231144646761
    },
    {
      "slice_name": "Reading Pace=very_long",
      "slice_dimension": "Reading Pace",
      "slice_value": "very_long",
      "count": 188973,
      "mae": 0.5101339195479728,
      "rmse": 0.8948480376729602,
      "mean_predicted": -1.7012685098084064,
      "mean_actual": -1.7543796456681122,
      "mean_error": -0.053111135859710376,
      "std_error": 0.8932728811869185
    },
    {
      "slice_name": "Author Gender=Female",
      "slice_dimension": "Author Gender",
      "slice_value": "Female",
      "count": 1140863,
      "mae": 0.4340016507493008,
      "rmse": 0.8198078494432641,
      "mean_predicted": -2.1563155217867194,
      "mean_actual": -2.2138466366311333,
      "mean_error": -0.057531114844414435,
      "std_error": 0.8177870548224075
    },
    {
      "slice_name": "Author Gender=Male",
      "slice_dimension": "Author Gender",
      "slice_value": "Male",
      "count": 1355575,
      "mae": 0.4787423352964836,
      "rmse": 0.8479262488724987,
      "mean_predicted": -1.8038510075980645,
      "mean_actual": -1.8479922211323183,
      "mean_error": -0.044141213534259115,
      "std_error": 0.8467768334953552
    },
    {
      "slice_name": "Author Gender=Unknown",
      "slice_dimension": "Author Gender",
      "slice_value": "Unknown",
      "count": 362043,
      "mae": 0.42043857907188903,
      "rmse": 0.8059577580003182,
      "mean_predicted": -2.248012379894304,
      "mean_actual": -2.3051248574669243,
      "mean_error": -0.057112477572622465,
      "std_error": 0.8039327445471982
    },
    {
      "slice_name": "Rating Range=Low (1-3)",
      "slice_dimension": "Rating Range",
      "slice_value": "Low (1-3)",
      "count": 2858468,
      "mae": 0.4534888047805614,
      "rmse": 0.8315220470798279,
      "mean_predicted": -2.0007892942992105,
      "mean_actual": -2.0519321416695693,
      "mean_error": -0.05114284737035389,
      "std_error": 0.8299479290387236
    },
    {
      "slice_name": "Rating Range=Medium (3-4)",
      "slice_dimension": "Rating Range",
      "slice_value": "Medium (3-4)",
      "count": 13,
      "mae": 3.1672390591676125,
      "rmse": 3.17135951312679,
      "mean_predicted": -0.0691010860296396,
      "mean_actual": 3.0981379731379732,
      "mean_error": 3.1672390591676125,
      "std_error": 0.16820938008351927
    }
  ],
  "disparity_analysis": {
    "summary": {
      "Popularity": {
        "mae_range": 0.18127228654078476,
        "rmse_range": 0.18599711911156058,
        "mae_coefficient_of_variation": 0.1460037641223395,
        "num_slices": 3,
        "max_mae_slice": "Medium",
        "min_mae_slice": "Low"
      },
      "Book Length": {
        "mae_range": 0.10695808629818876,
        "rmse_range": 0.10583263206169335,
        "mae_coefficient_of_variation": 0.08111624519898851,
        "num_slices": 4,
        "max_mae_slice": "very_long",
        "min_mae_slice": "medium"
      },
      "Book Era": {
        "mae_range": 0.19670575282392383,
        "rmse_range": 0.06560110589124879,
        "mae_coefficient_of_variation": 0.19519510548500513,
        "num_slices": 4,
        "max_mae_slice": "classic",
        "min_mae_slice": "recent"
      },
      "Genre Diversity": {
        "mae_range": 0.009471244598188988,
        "rmse_range": 0.03551556067579764,
        "mae_coefficient_of_variation": 0.010552537116997223,
        "num_slices": 2,
        "max_mae_slice": "Multi-genre",
        "min_mae_slice": "Some genres"
      },
      "Reading Pace": {
        "mae_range": 0.13466637188366565,
        "rmse_range": 0.15153006649110423,
        "mae_coefficient_of_variation": 0.11910639306175151,
        "num_slices": 4,
        "max_mae_slice": "very_long",
        "min_mae_slice": "quick_read"
      },
      "Author Gender": {
        "mae_range": 0.05830375622459455,
        "rmse_range": 0.041968490872180464,
        "mae_coefficient_of_variation": 0.056056050808376584,
        "num_slices": 3,
        "max_mae_slice": "Male",
        "min_mae_slice": "Unknown"
      },
      "Rating Range": {
        "mae_range": 2.713750254387051,
        "rmse_range": 2.339837466046962,
        "mae_coefficient_of_variation": 0.749504065579753,
        "num_slices": 2,
        "max_mae_slice": "Medium (3-4)",
        "min_mae_slice": "Low (1-3)"
      }
    },
    "detailed_disparities": [
      {
        "dimension": "Book Era",
        "severity": "medium",
        "mae_range": 0.19670575282392383,
        "mae_cv": 0.19519510548500513,
        "recommendation": "Significant performance disparity detected in Book Era. Consider bias mitigation."
      },
      {
        "dimension": "Rating Range",
        "severity": "high",
        "mae_range": 2.713750254387051,
        "mae_cv": 0.749504065579753,
        "recommendation": "Significant performance disparity detected in Rating Range. Consider bias mitigation."
      }
    ],
    "high_risk_slices": [
      {
        "slice": "Rating Range=Medium (3-4)",
        "dimension": "Rating Range",
        "mae": 3.1672390591676125,
        "mae_deviation_pct": 74.9504065579753,
        "count": 13
      }
    ]
  },
  "recommendations": [
    "HIGH PRIORITY: 1 dimensions show severe performance disparities. Dimensions: Rating Range",
    "MEDIUM PRIORITY: 1 dimensions show moderate disparities. Dimensions: Book Era",
    "Target these high-error slices for mitigation: Rating Range=Medium (3-4) (MAE: 3.167)",
    "Rating Range: Consider re-weighting training data or adjusting decision thresholds to balance performance across 2 groups"
  ]
}